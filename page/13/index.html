<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"louis-me.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":true,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="一个正经的测试工程师">
<meta property="og:type" content="website">
<meta property="og:title" content="施坤的博客">
<meta property="og:url" content="https://louis-me.github.io/page/13/index.html">
<meta property="og:site_name" content="施坤的博客">
<meta property="og:description" content="一个正经的测试工程师">
<meta property="og:locale" content="zh_CN">
<meta property="article:tag" content="自动化测试,安全测试,性能测试,渗透测试,大数据测试,python,jmeter,airtest,appium,monkey">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://louis-me.github.io/page/13/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>施坤的博客 </title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">施坤的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-简历">

    <a href="/resume/" rel="section"><i class="fa fa-address-card fa-fw"></i>简历</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/ae85ee2e/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/ae85ee2e/" class="post-title-link" itemprop="url">第四章 大数据之hive搭建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-07 16:56:19" itemprop="dateCreated datePublished" datetime="2021-12-07T16:56:19+08:00">2021-12-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span id="/aposts/ae85ee2e/" class="post-meta-item leancloud_visitors" data-flag-title="第四章 大数据之hive搭建" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/ae85ee2e/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/ae85ee2e/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul>
<li>学习本教程，请先看完<a target="_blank" rel="noopener" href="https://moon-full.gitee.io/2021/12/07/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop%E6%90%AD%E5%BB%BA/">第三章 大数据之Hadoop搭建</a></li>
<li>本次教程主要来自<a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1080-2/">基于Hadoop的数据仓库Hive 学习指南</a>，本次的内容全部经过了自己的实践，与参考文档中不一致的地方，也是经过查询各种资料和实践通过</li>
</ul>
<h2 id="hive简单介绍"><a href="#hive简单介绍" class="headerlink" title="hive简单介绍"></a>hive简单介绍</h2><p>使用 hive 的命令行接口，感觉很像操作关系数据库，但是 hive 和关系数据库还是有很大的不同，下面我就比较下 hive 与关系数据库的区别，具体如下：</p>
<ul>
<li>Hive 和关系数据库存储文件的系统不同，Hive 使用的是 hadoop 的 HDFS（hadoop 的分布式文件系统），关系数据库则是服务器本地的文件系统；</li>
<li>hive 使用的计算模型是 mapreduce，而关系数据库则是自己设计的计算模型；</li>
<li>关系数据库都是为实时查询的业务进行设计的，而 Hive 则是为海量数据做数据挖掘设计的，实时性很差；实时性的区别导致 Hive 的应用场景和关系数据库有很大的不同；</li>
</ul>
<h2 id="安装hive"><a href="#安装hive" class="headerlink" title="安装hive"></a>安装hive</h2><ul>
<li>下载版本选择为3.1.2</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-24-13-centos <span class="built_in">local</span>]<span class="comment"># wget --no-check-certificate  https://dlcdn.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz</span></span><br><span class="line"></span><br><span class="line">sudo tar -zxvf ./apache-hive-3.1.2-bin.tar.gz -C /usr/local   <span class="comment"># 解压到/usr/local中</span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/</span><br><span class="line">sudo <span class="built_in">mv</span> apache-hive-3.1.2-bin hive       <span class="comment"># 将文件夹名改为hive</span></span><br><span class="line">sudo <span class="built_in">chown</span> -R hadoop:hadoop hive   <span class="comment"># 修改文件权限</span></span><br></pre></td></tr></table></figure>

<h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos local]$ vi ~/.bashrc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure>

<ul>
<li><p>生效环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>


</li>
<li><p><strong>修改<code>/usr/local/hive/conf</code>下的hive-site.xml</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/hive/conf</span><br><span class="line"><span class="built_in">mv</span> hive-default.xml.template hive-default.xml</span><br><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos conf]$ vi hive-site.xml</span><br><span class="line"></span><br><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span> standalone=<span class="string">&quot;no&quot;</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet <span class="built_in">type</span>=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Driver class name <span class="keyword">for</span> a JDBC metastore&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hive1234&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;记得在创建用户时，密码要和这个对应&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="安装和配置mysql"><a href="#安装和配置mysql" class="headerlink" title="安装和配置mysql"></a>安装和配置mysql</h2><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><ul>
<li>我之前已经装好了，省略此步骤</li>
</ul>
<h3 id="配置mysql"><a href="#配置mysql" class="headerlink" title="配置mysql"></a>配置mysql</h3><ul>
<li>新建hive数据库</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos conf]$ mysql -u root -p</span><br><span class="line">create database hive;  #这个hive数据库与hive-site.xml中localhost:3306/hive的hive对应，用来保存hive元数据</span><br></pre></td></tr></table></figure>

<ul>
<li>配置mysql允许hive接入</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global validate_password.policy=<span class="string">&#x27;LOW&#x27;</span>; <span class="comment"># mysql8后，有密码策略要求，改为低</span></span><br><span class="line">mysql &gt;create user hive@localhost identified by <span class="string">&#x27;hive1234&#x27;</span>;</span><br><span class="line">					<span class="comment"># hive 代表你要创建的此数据库的新用户账号</span></span><br><span class="line">					<span class="comment"># localhost 代表访问本地权限，不可远程访问，还有其他值</span></span><br><span class="line">						<span class="comment"># %代表通配所有host地址权限(可远程访问)</span></span><br><span class="line">						<span class="comment"># 指定特殊Ip访问权限 如10.138.106.10</span></span><br><span class="line">                    <span class="comment"># hive1234代表你要创建的此数据库的新用密码</span></span><br><span class="line">mysql&gt;grant all privileges on  *.* to <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;%&#x27;</span> <span class="comment"># 授权数据库给hive用户</span></span><br><span class="line">mysql&gt; flush privileges;  <span class="comment">#刷新mysql系统权限关系表</span></span><br></pre></td></tr></table></figure>

<h2 id="配置hive"><a href="#配置hive" class="headerlink" title="配置hive"></a>配置hive</h2><h3 id="启动hive"><a href="#启动hive" class="headerlink" title="启动hive"></a>启动hive</h3><ul>
<li><p>启动hive之前，请先启动hadoop集群。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh <span class="comment">#启动hadoop</span></span><br><span class="line">hive  <span class="comment">#启动hive</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;<span class="comment"># 输入后报错</span></span><br><span class="line">FAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br></pre></td></tr></table></figure>

<ul>
<li>Hive现在包含一个用于 Hive Metastore 架构操控的脱机工具，名为 schematool.此工具可用于初始化当前 Hive 版本的 Metastore 架构。此外，其还可处理从较旧版本到新版本的架构升级，用下面的命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos conf]$ schematool -dbType mysql -initSchema</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出现下面的错误，提升驱动载入失败</span></span><br><span class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to load driver</span><br><span class="line">Underlying cause: java.lang.ClassNotFoundException : com.mysql.jdbc.Driver</span><br></pre></td></tr></table></figure>

<ul>
<li>打开<a target="_blank" rel="noopener" href="https://downloads.mysql.com/archives/c-j/">mysql下载链接</a></li>
</ul>
<p><img src="/aposts/ae85ee2e/image-20211208112332826.png" alt="image-20211208112332826"></p>
<ul>
<li>下载安装mysql驱动</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">sudo wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-8.0.26-1.el7.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到其他都是文档，那么连接驱动文件是/usr/share/java/mysql-connector-java.jar</span></span><br><span class="line">[root@study1 opt]<span class="comment"># rpm -qpl mysql-connector-java-8.0.26-1.el7.noarch.rpm </span></span><br><span class="line">警告：mysql-connector-java-8.0.26-1.el7.noarch.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</span><br><span class="line">/usr/share/doc/mysql-connector-java-8.0.26</span><br><span class="line">/usr/share/doc/mysql-connector-java-8.0.26/CHANGES</span><br><span class="line">/usr/share/doc/mysql-connector-java-8.0.26/INFO_BIN</span><br><span class="line">/usr/share/doc/mysql-connector-java-8.0.26/INFO_SRC</span><br><span class="line">/usr/share/doc/mysql-connector-java-8.0.26/LICENSE</span><br><span class="line">/usr/share/doc/mysql-connector-java-8.0.26/README</span><br><span class="line">/usr/share/java/mysql-connector-java.jar</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 安装，提示缺少依赖java-headless且版本要大于1.8版本</span></span><br><span class="line">[root@study1 opt]<span class="comment"># rpm -ivh mysql-connector-java-8.0.26-1.el7.noarch.rpm </span></span><br><span class="line">警告：mysql-connector-java-8.0.26-1.el7.noarch.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</span><br><span class="line">错误：依赖检测失败：</span><br><span class="line">	java-headless &gt;= 1:1.8.0 被 mysql-connector-java-1:8.0.26-1.el7.noarch 需要</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">[root@study1 opt]<span class="comment"># yum -y install java-headless</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 再次安装</span></span><br><span class="line">[root@study1 opt]<span class="comment"># rpm -ivh mysql-connector-java-8.0.26-1.el7.noarch.rpm </span></span><br><span class="line">警告：mysql-connector-java-8.0.26-1.el7.noarch.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</span><br><span class="line">准备中...                          <span class="comment">################################# [100%]</span></span><br><span class="line">正在升级/安装...</span><br><span class="line">   1:mysql-connector-java-1:8.0.26-1.e<span class="comment">################################# [100%]</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 查看驱动文件</span></span><br><span class="line">[root@study1 opt]<span class="comment"># ll /usr/share/java</span></span><br><span class="line">总用量 2328</span><br><span class="line">-rw-r--r--. 1 root root 2381198 9月  11 05:55 mysql-connector-java.jar</span><br><span class="line"><span class="comment"># 把驱动文件拷贝到hive的lib中</span></span><br><span class="line">[root@study1 opt]<span class="comment"># cp /usr/share/java/mysql-connector-java.jar /usr/local/hive/lib</span></span><br></pre></td></tr></table></figure>

<ul>
<li>再次schematool初始化就可以了</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos local]$ schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure>

<ul>
<li>进入hive</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos <span class="built_in">local</span>]$ hive</span><br><span class="line"></span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.02 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.036 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>如果要退出Hive交互式执行环境，可以输入如下命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="built_in">exit</span>;</span><br><span class="line">[hadoop@VM-24-13-centos <span class="built_in">local</span>]$</span><br></pre></td></tr></table></figure>

<h2 id="Hive的常用HiveQL操作"><a href="#Hive的常用HiveQL操作" class="headerlink" title="Hive的常用HiveQL操作"></a>Hive的常用HiveQL操作</h2><h3 id="Hive基本数据类型"><a href="#Hive基本数据类型" class="headerlink" title="Hive基本数据类型"></a>Hive基本数据类型</h3><ul>
<li>Hive支持基本数据类型和复杂类型, 基本数据类型主要有数值类型(INT、FLOAT、DOUBLE ) 、布尔型和字符串, 复杂类型有三种:ARRAY、MAP 和 STRUCT。</li>
</ul>
<h4 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h4><ul>
<li>TINYINT: 1个字节</li>
<li>SMALLINT: 2个字节</li>
<li>INT: 4个字节</li>
<li>BIGINT: 8个字节</li>
<li>BOOLEAN: TRUE&#x2F;FALSE</li>
<li>FLOAT: 4个字节，单精度浮点型</li>
<li>DOUBLE: 8个字节，双精度浮点型STRING 字符串</li>
</ul>
<h4 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h4><ul>
<li>ARRAY: 有序字段</li>
<li>MAP: 无序字段</li>
<li>STRUCT: 一组命名的字段</li>
</ul>
<h3 id="用的HiveQL操作命令"><a href="#用的HiveQL操作命令" class="headerlink" title="用的HiveQL操作命令"></a>用的HiveQL操作命令</h3><ul>
<li><p>Hive常用的HiveQL操作命令主要包括：数据定义、数据操作。接下来详细介绍一下这些命令即用法（想要了解更多请参照《Hive编程指南》一书）</p>
</li>
<li><p>数据定义：主要用于创建修改和删除数据库、表、视图、函数和索引。</p>
</li>
</ul>
<h4 id="创建、修改和删除数据库"><a href="#创建、修改和删除数据库" class="headerlink" title="创建、修改和删除数据库"></a>创建、修改和删除数据库</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists hive;       #创建数据库</span><br><span class="line">show databases;                           #查看Hive中包含数据库</span><br><span class="line">show databases like &#x27;h.*&#x27;;                #查看Hive中以h开头数据库</span><br><span class="line">use hive; # 使用数据库</span><br><span class="line">show tables; # 查看表列表</span><br><span class="line">drop table usr; #  删除表</span><br></pre></td></tr></table></figure>

<h4 id="创建、修改和删除表"><a href="#创建、修改和删除表" class="headerlink" title="创建、修改和删除表"></a>创建、修改和删除表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#创建内部表（管理表）</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr(</span><br><span class="line">      name string comment <span class="string">&#x27;username&#x27;</span>, # name表示字段命，string表示字段类型，comment后面内容表示说明</span><br><span class="line">      pwd string comment <span class="string">&#x27;password&#x27;</span>,</span><br><span class="line">      address struct<span class="operator">&lt;</span>street:string,city:string,state:string,zip:<span class="type">int</span><span class="operator">&gt;</span> comment  <span class="string">&#x27;home address&#x27;</span>,</span><br><span class="line">      identify map<span class="operator">&lt;</span><span class="type">int</span>,tinyint<span class="operator">&gt;</span> comment <span class="string">&#x27;number,sex&#x27;</span>) comment <span class="string">&#x27;description of the table&#x27;</span>  </span><br><span class="line">     tblproperties(<span class="string">&#x27;creator&#x27;</span><span class="operator">=</span><span class="string">&#x27;me&#x27;</span>,<span class="string">&#x27;time&#x27;</span><span class="operator">=</span><span class="string">&#x27;2016.1.1&#x27;</span>); #tblproperties 设置表的属性 </span><br><span class="line">     </span><br><span class="line">#创建外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> usr2(</span><br><span class="line">      name string,</span><br><span class="line">      pwd string,</span><br><span class="line">  address struct<span class="operator">&lt;</span>street:string,city:string,state:string,zip:<span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">      identify map<span class="operator">&lt;</span><span class="type">int</span>,tinyint<span class="operator">&gt;</span>) </span><br><span class="line">      <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> # 字段分隔符来进行分割，如：test1,<span class="number">223456</span>,湖南省</span><br><span class="line">     location <span class="string">&#x27;/usr/local/hive/warehouse/hive.db/usr&#x27;</span>; </span><br><span class="line">     # LOCATION一般与外部表（<span class="keyword">EXTERNAL</span>）一起使用。一般情况下hive元数据默认保存在<span class="operator">&lt;</span>hive.metastore.warehouse.dir<span class="operator">&gt;</span>中。</span><br><span class="line">     # 这个字段的适用场景是：数据已经存在HDFS上不能移动位置了，那么就通过这个字段让表可以直接读到这份数据。另外，要注意建表的时候，应该让表变成外部表。</span><br><span class="line"></span><br><span class="line">#创建分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> usr3(</span><br><span class="line">      name string,</span><br><span class="line">      pwd string,</span><br><span class="line">      address struct<span class="operator">&lt;</span>street:string,city:string,state:string,zip:<span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">      identify map<span class="operator">&lt;</span><span class="type">int</span>,tinyint<span class="operator">&gt;</span>) </span><br><span class="line">      partitioned <span class="keyword">by</span>(city string,state string); # 双分区</span><br><span class="line">      </span><br><span class="line"> #复制usr表的表模式       </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr1 <span class="keyword">like</span> hive.usr;</span><br><span class="line"><span class="keyword">show</span> tables <span class="keyword">in</span> hive;  </span><br><span class="line"><span class="keyword">show</span> tables <span class="string">&#x27;u.*&#x27;</span>;        #查看hive中以u开头的表</span><br><span class="line"><span class="keyword">describe</span> hive.usr;        #查看usr表相关信息</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hive.usr rename <span class="keyword">to</span> custom;      #重命名表</span><br><span class="line">#为表增加一个分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(city<span class="operator">=</span>&quot;beijing&quot;,state<span class="operator">=</span>&quot;China&quot;) location <span class="string">&#x27;/usr/local/hive/warehouse/usr3/China/beijing&#x27;</span>; </span><br><span class="line"></span><br><span class="line">#修改分区路径</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">partition</span>(city<span class="operator">=</span>&quot;beijing&quot;,state<span class="operator">=</span>&quot;China&quot;) <span class="keyword">set</span> location <span class="string">&#x27;/usr/local/hive/warehouse/usr3/CH/beijing&#x27;</span>;</span><br><span class="line">#删除分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">drop</span> if <span class="keyword">exists</span>  <span class="keyword">partition</span>(city<span class="operator">=</span>&quot;beijing&quot;,state<span class="operator">=</span>&quot;China&quot;);</span><br><span class="line">#修改列信息,注意这里，如果使用 after时，交换元素类型不一致，就无法交换成功</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom change <span class="keyword">column</span> username username string after pwd;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom <span class="keyword">add</span> columns(hobby string);                  #增加列</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom replace columns(uname string);              #删除替换列</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom <span class="keyword">set</span> tblproperties(<span class="string">&#x27;creator&#x27;</span><span class="operator">=</span><span class="string">&#x27;liming&#x27;</span>);      #修改表属性</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">partition</span>(city<span class="operator">=</span>&quot;beijing&quot;,state<span class="operator">=</span>&quot;China&quot;) <span class="keyword">set</span> fileformat sequencefile;     #修改存储属性            </span><br><span class="line">use hive;                                                   #切换到hive数据库下</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> usr1;                                  #删除表</span><br><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> hive cascade;                       #删除数据库和它中的表</span><br></pre></td></tr></table></figure>

<h6 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h6><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sx66/p/12039163.html">Hive的数据模型及各模块的应用场景</a> 结合这个<a target="_blank" rel="noopener" href="https://www.cnblogs.com/syx-1987/p/4182967.html">Hive之数据模型</a>来看效果比较好</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/heiren_a/article/details/109119081">Hive在建表时的分隔符的设置</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44736028/article/details/106250453">hive 中的location</a></p>
</li>
</ul>
<h4 id="视图和索引的创建、修改和删除"><a href="#视图和索引的创建、修改和删除" class="headerlink" title="视图和索引的创建、修改和删除"></a>视图和索引的创建、修改和删除</h4><ul>
<li>主要语法如下，用户可自行实现。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create view view_name as....;                <span class="comment">#创建视图</span></span><br><span class="line">alter view view_name <span class="built_in">set</span> tblproperties(…);   <span class="comment">#修改视图</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>因为视图是只读的，所以 对于视图只允许改变元数据中的 tblproperties属性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#删除视图</span><br><span class="line">drop view if exists view_name;</span><br><span class="line">#创建索引</span><br><span class="line">create index index_name on table table_name(partition_name/column_name)  </span><br><span class="line">as &#x27;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#x27; with deferred rebuild....; </span><br></pre></td></tr></table></figure>
</li>
<li><p>这里’org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’是一个索引处理器，即一个实现了索引接口的Java类，另外Hive还有其他的索引实现。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter index index_name on table table_name partition(...)  rebulid;   #重建索引</span><br></pre></td></tr></table></figure>

<ul>
<li>如果使用 deferred rebuild，那么新索引成空白状态，任何时候可以进行第一次索引创建或重建。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show formatted index on table_name;                       #显示索引</span><br><span class="line">drop index if exists index_name on table table_name;      #删除索引</span><br></pre></td></tr></table></figure>

<h6 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h6><ul>
<li>这里没有实践，示例可以参考这里<a target="_blank" rel="noopener" href="https://www.cnblogs.com/tsxylhs/p/7341474.html">hadoop Hive 的建表 和导入导出及索引视图</a></li>
</ul>
<h4 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h4><ul>
<li><p>没有实践自定义函数，后续有需求在学</p>
</li>
<li><p>在新建用户自定义函数（UDF）方法前，先了解一下Hive自带的那些函数。<code>show functions;</code> 命令会显示Hive中所有的函数名称：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show functions;</span><br><span class="line">OK</span><br><span class="line">!</span><br><span class="line">!=</span><br><span class="line">$sum0</span><br><span class="line">....</span><br><span class="line">cardinality_violation</span><br><span class="line">case</span><br><span class="line">cbrt</span><br><span class="line">ceil</span><br><span class="line">ceiling</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ul>
<li>若想要查看具体函数使用方法可使用describe function 函数名：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; describe function abs;</span><br><span class="line">OK</span><br><span class="line">abs(x) - returns the absolute value of x</span><br></pre></td></tr></table></figure>

<ul>
<li><p>首先编写自己的UDF前需要继承UDF类并实现evaluate()函数，或是继承GenericUDF类实现initialize()函数、evaluate()函数和getDisplayString()函数，还有其他的实现方法，感兴趣的用户可以自行学习。</p>
</li>
<li><p>另外，如果用户想在Hive中使用该UDF需要将我们编写的Java代码进行编译，然后将编译后的UDF二进制类文件(.class文件)打包成一个JAR文件，然后在Hive会话中将这个JAR文件加入到类路径下，在通过create function语句定义好使用这个Java类的函数。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add jar &lt;jar文件的绝对路径&gt;;                        #创建函数</span><br><span class="line">create temporary function function_name;</span><br><span class="line">drop temporary function if exists function_name;    #删除函数</span><br></pre></td></tr></table></figure>

<h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><ul>
<li>主要实现的是将数据装载到表中（或是从表中导出），并进行相应查询操作，对熟悉SQL语言的用户应该不会陌生。</li>
</ul>
<h4 id="向表中装载数据"><a href="#向表中装载数据" class="headerlink" title="向表中装载数据"></a>向表中装载数据</h4><p>这里我们以只有两个属性的简单表为例来介绍。首先创建表stu和course，stu有两个属性id与name，course有两个属性cid与sid。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists hive.stu(id int,name string) </span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">create table if not exists hive.course(cid int,sid int) </span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<ul>
<li>向表中装载数据有两种方法：<strong>从文件中导入和通过查询语句插入</strong>。</li>
</ul>
<h5 id="从文件中导入"><a href="#从文件中导入" class="headerlink" title="从文件中导入"></a>从文件中导入</h5><ul>
<li>假如这个表中的记录存储于文件stu.txt中，该文件的存储路径为&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;examples&#x2F;stu.txt，内容如下。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># id后面的name分割符号要用键盘上的tab隔开，直接复制过去，导入进去会全部都是null</span><br><span class="line">1	xiapi </span><br><span class="line">2	xiaoxue </span><br><span class="line">3	qingqing</span><br></pre></td></tr></table></figure>

<ul>
<li><p>下面我们把这个文件中的数据装载到表stu中，操作如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> use hive;</span><br><span class="line">hive<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/hadoop/examples/stu.txt&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> hive.stu</span><br><span class="line">OK</span><br><span class="line"># 查询到数据</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hive.stu;</span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>       xiapi</span><br><span class="line"><span class="number">2</span>       xiaoxue</span><br><span class="line"><span class="number">3</span>       qingqing</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.324</span> s</span><br></pre></td></tr></table></figure>

<ul>
<li>如果stu.txt文件存储在HDFS 上，则不需要 local 关键字。</li>
</ul>
</li>
</ul>
<h5 id="通过查询语句插入"><a href="#通过查询语句插入" class="headerlink" title="通过查询语句插入"></a>通过查询语句插入</h5><p>使用如下命令，创建stu1表，它和stu表属性相同，我们要把从stu表中查询得到的数据插入到stu1中：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> stu1 <span class="keyword">as</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> stu;</span><br><span class="line">...</span><br><span class="line">Moving data <span class="keyword">to</span> directory hdfs:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">9000</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>hive.db<span class="operator">/</span>.hive<span class="operator">-</span>staging_hive_2021<span class="number">-12</span><span class="number">-10</span>_10<span class="number">-42</span><span class="number">-32</span>_320_1543053100774530944<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>ext<span class="number">-10002</span></span><br><span class="line">Moving data <span class="keyword">to</span> directory hdfs:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">9000</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>hive.db<span class="operator">/</span>stu1</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>:  HDFS Read: <span class="number">31</span> HDFS Write: <span class="number">114</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">0</span> msec</span><br><span class="line">OK</span><br><span class="line"># 查询到stu1表结构如下</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">describe</span> stu1;</span><br><span class="line">OK</span><br><span class="line">id                      <span class="type">int</span></span><br><span class="line">name                    string</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.268</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<p>上面是创建表，并直接向新表插入数据；若表已经存在，向表中插入数据需执行以下命令：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 这里关键字overwrite的作用是替换掉表（或分区）中原有数据，换成<span class="keyword">into</span>关键字，直接追加到原有内容后。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu1 <span class="keyword">select</span> id,name <span class="keyword">from</span> stu <span class="keyword">where</span>（id<span class="operator">=</span><span class="string">&#x27;1&#x27;</span>);</span><br><span class="line"></span><br><span class="line"># 查询发现只有id为<span class="number">1</span>的数据，其他数据全部清空</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hive.stu1;</span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>       xiapi</span><br></pre></td></tr></table></figure>

<h4 id="从表中导出数据"><a href="#从表中导出数据" class="headerlink" title="从表中导出数据"></a>从表中导出数据</h4><h5 id="导出到本地文件"><a href="#导出到本地文件" class="headerlink" title="导出到本地文件"></a>导出到本地文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">hive&gt; </span><span class="language-bash">insert overwrite <span class="built_in">local</span> directory <span class="string">&#x27;/usr/local/hadoop/examples/export_stu&#x27;</span> select * from hive.stu;</span></span><br><span class="line">...</span><br><span class="line">Moving data to local directory /usr/local/hadoop/examples/export_stu</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage-Stage-1:  HDFS Read: 30 HDFS Write: 0 SUCCESS</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看导出的文件</span></span><br><span class="line">[hadoop@VM-24-13-centos local]$ cat /usr/local/hadoop/examples/export_stu/000000_0</span><br><span class="line">1xiapi</span><br><span class="line">2xiaoxue</span><br><span class="line">3qingqing</span><br></pre></td></tr></table></figure>

<h5 id="导出到hdfs"><a href="#导出到hdfs" class="headerlink" title="导出到hdfs"></a>导出到hdfs</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert overwrite directory &#x27;/usr/local/hadoop/examples/export_hdfs_stu&#x27; select * from hive.stu;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Moving data to directory /usr/local/hadoop/examples/export_hdfs_stu</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage-Stage-1:  HDFS Read: 30 HDFS Write: 30 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line"># 查看导出成功的数据</span><br><span class="line">hive&gt; dfs -cat /usr/local/hadoop/examples/export_hdfs_stu/*;</span><br><span class="line">1xiapi</span><br><span class="line">2xiaoxue</span><br><span class="line">3qingqing</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="查询操作"><a href="#查询操作" class="headerlink" title="查询操作"></a>查询操作</h4><ul>
<li><p>和SQL的查询完全一样，这里不再赘述。主要使用select…from…where…等语句，再结合关键字group by、having、like、rlike等操作。这里我们简单介绍一下SQL中没有的case…when…then…句式、join操作和子查询操作。</p>
</li>
<li><p><code>case…when…then…</code>句式和if条件语句类似，用于处理单个列的查询结果，语句如下：</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> id,name,</span><br><span class="line">    <span class="operator">&gt;</span>   <span class="keyword">case</span></span><br><span class="line">    <span class="operator">&gt;</span>   <span class="keyword">when</span> id<span class="operator">=</span><span class="number">1</span> <span class="keyword">then</span> <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    <span class="operator">&gt;</span>   <span class="keyword">when</span> id<span class="operator">=</span><span class="number">2</span> <span class="keyword">then</span> <span class="string">&#x27;second&#x27;</span></span><br><span class="line">    <span class="operator">&gt;</span>   <span class="keyword">else</span> <span class="string">&#x27;third&#x27;</span></span><br><span class="line">    <span class="operator">&gt;</span>   <span class="keyword">end</span> <span class="keyword">from</span> stu;</span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>       xiapi   <span class="keyword">first</span></span><br><span class="line"><span class="number">2</span>       xiaoxue         <span class="keyword">second</span></span><br><span class="line"><span class="number">3</span>       qingqing        third</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.385</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<ul>
<li>连接（join）是将两个表中在共同数据项上相互匹配的那些行合并起来, HiveQL 的连接分为内连接、左向外连接、右向外连接、全外连接和半连接 5 种。</li>
</ul>
<h5 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h5><ul>
<li>内连接使用比较运算符根据每个表共有的列的值匹配两个表中的行。</li>
<li>首先，我们先把以下内容插入到course表中</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> course <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> course <span class="keyword">values</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> course <span class="keyword">values</span>(<span class="number">3</span>,<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li><p>下面, 查询stu和course表中学号相同的所有行，命令如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> stu.<span class="operator">*</span>, course.<span class="operator">*</span> <span class="keyword">from</span> stu <span class="keyword">join</span> course <span class="keyword">on</span>(stu.id<span class="operator">=</span>course.sid);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>       xiapi   <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"><span class="number">1</span>       xiapi   <span class="number">3</span>       <span class="number">1</span></span><br><span class="line"><span class="number">2</span>       xiaoxue         <span class="number">1</span>       <span class="number">2</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">11.049</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="右连接"><a href="#右连接" class="headerlink" title="右连接"></a>右连接</h5><p>右连接是左向外连接的反向连接,将返回右表的所有行。如果右表的某行在左表中没有匹配行,则将为左表返回空值。命令</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> stu.<span class="operator">*</span>, course.<span class="operator">*</span> <span class="keyword">from</span> stu <span class="keyword">right</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu.id<span class="operator">=</span>course.sid);</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">0</span> msec</span><br><span class="line">OK</span><br><span class="line"><span class="number">2</span>       xiaoxue         <span class="number">1</span>       <span class="number">2</span></span><br><span class="line"><span class="number">1</span>       xiapi   <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"><span class="number">1</span>       xiapi   <span class="number">3</span>       <span class="number">1</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">10.887</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<h5 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h5><p>全连接返回左表和右表中的所有行。当某行在另一表中没有匹配行时,则另一个表的选择列表包含空值。如果表之间有匹配行,则整个结果集包含基表的数据值。命令如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> stu.<span class="operator">*</span>, course.<span class="operator">*</span> <span class="keyword">from</span> stu <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id<span class="operator">=</span>course .sid);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>       xiapi   <span class="number">3</span>       <span class="number">1</span></span><br><span class="line"><span class="number">1</span>       xiapi   <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"><span class="number">2</span>       xiaoxue         <span class="number">1</span>       <span class="number">2</span></span><br><span class="line"><span class="number">3</span>       qingqing        <span class="keyword">NULL</span>    <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure>

<h5 id="半连接"><a href="#半连接" class="headerlink" title="半连接"></a>半连接</h5><p>半连接是 Hive 所特有的, <strong>Hive 不支持 in 操作,但是拥有替代的方案</strong>;<code> left semi join,</code> 称为半连接, 需要注意的是连接的表不能在查询的列中,只能出现在 on 子句中。命令如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> stu.<span class="operator">*</span> <span class="keyword">from</span> stu <span class="keyword">left</span> semi <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id<span class="operator">=</span>course .sid);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">0</span> msec</span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>       xiapi</span><br><span class="line"><span class="number">2</span>       xiaoxue</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">9.267</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure>

<h5 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h5><p>标准 SQL 的子查询支持嵌套的 select 子句,HiveQL 对子查询的支持很有限,只能在from 引导的子句中出现子查询。</p>
<h3 id="Hive简单编程实践"><a href="#Hive简单编程实践" class="headerlink" title="Hive简单编程实践"></a>Hive简单编程实践</h3><ul>
<li><p>下面我们以词频统计算法为例，来介绍怎么在具体应用中使用Hive。词频统计算法又是最能体现MapReduce思想的算法之一，这里我们可以对比它在MapReduce中的实现，来说明使用Hive后的优势。</p>
</li>
<li><p>MapReduce实现词频统计的代码可以通过下载Hadoop源码后，在 $HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.0.3.jar 包中找到(wordcount类)，wordcount类由63行Java代码编写而成。下面首先简单介绍一下怎么使用MapReduce中wordcount类来统计单词出现的次数，具体步骤如下：</p>
<ul>
<li>创建input目录，output目录会自动生成。其中input为输入目录，output目录为输出目录。命令如下：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">rm -r input</span><br><span class="line">mkdir input</span><br></pre></td></tr></table></figure>

<ul>
<li>然后，在input文件夹中创建两个测试文件file1.txt和file2.txt，命令如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd  /usr/local/hadoop/input</span><br><span class="line">[hadoop@VM-24-13-centos input]$ echo &quot;hello world&quot; &gt; file1.txt</span><br><span class="line">[hadoop@VM-24-13-centos input]$ echo &quot;hello world&quot; &gt; file1.txt</span><br></pre></td></tr></table></figure>

<ul>
<li>执行如下hadoop命令：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ cd  ..</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先删除hdfs中output目录，不然会报错</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hadoop dfs -rm -r output</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> 删除input</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -rm -r input</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在HDFS创建一个目录</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -mkdir input</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">复制input到hdfs中的input目录</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -put input/*  input</span><br><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar wordcount input output</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">2021-12-10 15:22:03,703 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">2021-12-10 15:22:03,704 INFO mapreduce.Job: Job job_local1659054277_0001 completed successfully</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>我们可以到output文件夹中查看结果，结果如下：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -ls output/*</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-12-10 17:12 output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         25 2021-12-10 17:12 output/part-r-00000</span><br><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -cat  output/part-r-00000</span><br><span class="line">hadoop  1</span><br><span class="line">hello   2</span><br><span class="line">world   1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><p>下面我们通过HiveQL实现词频统计功能，此时只要编写下面7行代码，而且不需要进行编译生成jar来执行。HiveQL实现命令如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@VM</span><span class="number">-24</span><span class="number">-13</span><span class="operator">-</span>centos hadoop]$ hive</span><br><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> docs(line string);</span><br><span class="line">hive<span class="operator">&gt;</span> load data inpath <span class="string">&#x27;input&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> docs;</span><br><span class="line"># <span class="keyword">create</span> <span class="keyword">table</span> word_count 表示创建数据库</span><br><span class="line"># <span class="keyword">as</span> <span class="keyword">select</span> word, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> count 表示查询列表，一个word，一个统计值</span><br><span class="line">#  <span class="keyword">from</span> (<span class="keyword">select</span> explode(split(line,<span class="string">&#x27; &#x27;</span>))<span class="keyword">as</span> word <span class="keyword">from</span> docs) 这里就是从docs复制数据</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> word_count <span class="keyword">as</span> <span class="keyword">select</span> word, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> count <span class="keyword">from</span> (<span class="keyword">select</span> explode(split(line,<span class="string">&#x27; &#x27;</span>))<span class="keyword">as</span> word <span class="keyword">from</span> docs) w <span class="keyword">group</span> <span class="keyword">by</span> word <span class="keyword">order</span> <span class="keyword">by</span> word;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行后，用select语句查看，结果如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> word_count;</span><br><span class="line">OK</span><br><span class="line">hadoop  <span class="number">1</span></span><br><span class="line">hello   <span class="number">2</span></span><br><span class="line">world   <span class="number">1</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.148</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>由上可知，采用Hive实现最大的优势是，对于非程序员，不用学习编写Java MapReduce代码了，只需要用户学习使用HiveQL就可以了，而这对于有SQL基础的用户而言是非常容易的。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/d7dcf086/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/d7dcf086/" class="post-title-link" itemprop="url">第三章 大数据之Hadoop搭建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-07 09:18:51" itemprop="dateCreated datePublished" datetime="2021-12-07T09:18:51+08:00">2021-12-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span id="/aposts/d7dcf086/" class="post-meta-item leancloud_visitors" data-flag-title="第三章 大数据之Hadoop搭建" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/d7dcf086/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/d7dcf086/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>服务器信息，是腾讯云服务器，2核cpu，4GB内存，80GB云硬盘，系统为centos 7.6_x64</li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Hadoop是用来处理大数据集合的分布式存储计算基础架构。可以使用一种简单的编程模式，通过多台计算机构成的集群，分布式处理大数据集。hadoop作为底层，其生态环境很丰富。hadoop基础包括以下四个基本模块：</p>
<ul>
<li>hadoop基础功能库：支持其他hadoop模块的通用程序包。</li>
<li>HDFS: 一个分布式文件系统，能够以高吞吐量访问应用的数据。</li>
<li>YARN: 一个作业调度和资源管理框架。</li>
<li>MapReduce: 一个基于YARN的大数据并行处理程序。</li>
</ul>
<h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><h3 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su              <span class="comment">#  root 用户登录</span></span><br><span class="line">useradd -m hadoop -s /bin/bash   <span class="comment"># 创建新用户hadoop,并使用 /bin/bash 作为shell</span></span><br><span class="line">passwd hadoop <span class="comment"># 设置密码</span></span><br></pre></td></tr></table></figure>

<h4 id="设置权限"><a href="#设置权限" class="headerlink" title="设置权限"></a>设置权限</h4><ul>
<li><p>为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题，输入命令<code>visudo</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ sudo visudo</span><br><span class="line">[sudo] password for hadoop:</span><br></pre></td></tr></table></figure>


</li>
<li><p>找到 <code>root ALL=(ALL) ALL</code> 这行，然后在这行下面增加一行内容：<code>hadoop ALL=(ALL) ALL</code> （当中的间隔为tab），如下图所示：</p>
<p><img src="/aposts/d7dcf086/image-20211207093705705.png" alt="image-20211207093705705"></p>
</li>
<li><p><code>su hadoop</code> 直接可以切换用户</p>
</li>
</ul>
<h3 id="安装SSH、配置SSH无密码登陆"><a href="#安装SSH、配置SSH无密码登陆" class="headerlink" title="安装SSH、配置SSH无密码登陆"></a>安装SSH、配置SSH无密码登陆</h3><ul>
<li><p>集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），一般情况下，CentOS 默认已安装了 SSH client、SSH server，打开终端执行如下命令进行检验：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos /]$ rpm -qa | grep ssh</span><br><span class="line">openssh-7.4p1-21.el7.x86_64</span><br><span class="line">openssh-clients-7.4p1-21.el7.x86_64</span><br><span class="line">openssh-server-7.4p1-21.el7.x86_64</span><br><span class="line">libssh2-1.8.0-4.el7.x86_64</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>包含了 SSH client 跟 SSH server，则不需要再安装</li>
</ul>
</li>
<li><p>接着执行如下命令(<code>ssh localhost</code>)测试一下 SSH 是否可用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos /]$ ssh localhost</span><br><span class="line">The authenticity of host <span class="string">&#x27;localhost (::1)&#x27;</span> can<span class="string">&#x27;t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:v9LOJv5al8BNRGGZVJeqa2AdV3znIsa6cjyoj9CbWRQ.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is MD5:bd:51:9d:6f:1f:9c:1f:ad:34:ce:fb:90:4f:27:bc:b1.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>此时会有如下提示(SSH首次登陆提示)，输入 yes 。然后按提示输入密码 hadoop，这样就登陆到本机了，类似于下面这样：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Warning: Permanently added <span class="string">&#x27;localhost&#x27;</span> (ECDSA) to the list of known hosts.</span><br><span class="line">hadoop@localhost<span class="string">&#x27;s password:</span></span><br><span class="line"><span class="string">Last login: Tue Dec  7 09:37:37 2021</span></span><br><span class="line"><span class="string">[hadoop@VM-24-13-centos ~]$</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。</p>
</li>
<li><p>首先输入 <code>exit</code> 退出刚才的 ssh，就回到了我们原先的终端窗口，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos ~]$ <span class="built_in">exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to localhost closed.</span><br><span class="line">[hadoop@VM-24-13-centos /]$</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh/                     <span class="comment"># 若没有该目录，请先执行一次ssh localhost</span></span><br><span class="line">ssh-keygen -t rsa              <span class="comment"># 会有提示，都按回车就可以</span></span><br><span class="line"><span class="built_in">cat</span> id_rsa.pub &gt;&gt; authorized_keys  <span class="comment"># 加入授权</span></span><br><span class="line"><span class="built_in">chmod</span> 600 ./authorized_keys    <span class="comment"># 修改文件权限</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>~的含义</strong></p>
<p>在 Linux 系统中，~ 代表的是用户的主文件夹，即 “&#x2F;home&#x2F;用户名” 这个目录，如你的用户名为 hadoop，则 ~ 就代表 “&#x2F;home&#x2F;hadoop&#x2F;”。 此外，命令中的 # 后面的文字是注释。</p>
</blockquote>
</li>
<li><p>此时再用 <code>ssh localhost</code> 命令，无需输入密码就可以直接登陆了，如下所示。</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop@VM-24-13-centos .ssh]$ ssh localhost</span><br><span class="line">Last login: Tue Dec  7 09:44:00 2021 from ::1</span><br><span class="line">[hadoop@VM-24-13-centos ~]$</span><br></pre></td></tr></table></figure>

<h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><ul>
<li><p>java之前已经装好了，为1.8</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos ~]$ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_311&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_311-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.311-b11, mixed mode)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h3><ul>
<li>安装hadoop版本为3.0</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">su root</span><br><span class="line"><span class="built_in">cd</span> /usr/local</span><br><span class="line">wget https://archive.apache.org/dist/hadoop/common/hadoop-3.0.3/hadoop-3.0.3.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos <span class="built_in">local</span>]$ sudo tar zxvf hadoop-3.0.3.tar.gz</span><br><span class="line"><span class="built_in">cd</span> /usr/local/</span><br><span class="line">sudo <span class="built_in">mv</span> ./hadoop-3.0.3/ ./hadoop            <span class="comment"># 将文件夹名改为hadoop</span></span><br><span class="line">sudo <span class="built_in">chown</span> -R hadoop:hadoop ./hadoop        <span class="comment"># 修改文件权限</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/hadoop</span><br><span class="line">./bin/hadoop version</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./bin/hadoop version</span><br><span class="line">Hadoop 3.0.3</span><br><span class="line">Source code repository https://yjzhangal@git-wip-us.apache.org/repos/asf/hadoop.git -r 37fd7d752db73d984dc31e0cdfd590d252f5e075</span><br><span class="line">Compiled by yzhang on 2018-05-31T17:12Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From <span class="built_in">source</span> with checksum 736cdcefa911261ad56d2d120bf1fa</span><br><span class="line">This <span class="built_in">command</span> was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.0.3.jar</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Hadoop单机配置-非分布式"><a href="#Hadoop单机配置-非分布式" class="headerlink" title="Hadoop单机配置(非分布式)"></a>Hadoop单机配置(非分布式)</h2><ul>
<li>Hadoop 默认模式为非分布式模式，无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。</li>
<li><strong>这里比较奇怪，安装原文中教程，示例已经验证通过了，当搭建了伪分布式配置时，第二天运行这里的实例，没有成功？然后把实例修改后，发现居然和伪分布式配置代码差不多？暂不做深究了</strong></li>
</ul>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><ul>
<li><p>现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（运行 <code>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar</code> 可以看到所有例子），包括 wordcount、terasort、join、grep 等。</p>
</li>
<li><p>在此我们选择运行 wordcount例子，我们将 input 文件夹中的所有文件作为输入，最后输出结果到 output 文件夹中。</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/hadoop</span><br><span class="line">mkdri iniput1</span><br><span class="line"><span class="built_in">cp</span> ./etc/hadoop/*.xml ./input1</span><br><span class="line"><span class="comment"># 在HDFS创建一个目录</span></span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> /usr/local/hadoop/input</span><br><span class="line"><span class="comment">#  将配置文件作为输入文件上传到刚创建的HDFS目录中</span></span><br><span class="line">hdfs dfs -put ./input1/* /usr/local/hadoop/input</span><br><span class="line"></span><br><span class="line">hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /usr/local/hadoop/input /usr/local/hadoop/output </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看运行结果列表</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -<span class="built_in">ls</span> /usr/local/hadoop/output/*</span><br><span class="line"></span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-12-10 16:48 /usr/local/hadoop/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       9405 2021-12-10 16:48 /usr/local/hadoop/output/part-r-00000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用cat查运行数据</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -<span class="built_in">cat</span> /usr/local/hadoop/output/part-r-00000</span><br><span class="line">...</span><br><span class="line">datanodes       1</span><br><span class="line">decryptEncryptedKey     1</span><br><span class="line">default 13</span><br><span class="line">default_priority=&#123;priority&#125;]    1</span><br><span class="line">defined.        4</span><br><span class="line">delete-key      1</span><br><span class="line">dfsadmin        1</span><br><span class="line">different   </span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>注意</strong>，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 <code>./output</code> 删除。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ hdfs dfs -<span class="built_in">rm</span> -r /usr/local/hadoop/output</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Hadoop伪分布式配置"><a href="#Hadoop伪分布式配置" class="headerlink" title="Hadoop伪分布式配置"></a>Hadoop伪分布式配置</h2><ul>
<li>Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，<strong>节点既作为 NameNode 也作为 DataNode，同时读取的是 HDFS 中的文件</strong>。</li>
</ul>
<h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><ul>
<li>在设置 Hadoop 伪分布式配置前，我们还需要设置 HADOOP 环境变量，执行如下命令在 ~&#x2F;.bashrc 中设置：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc</span><br><span class="line"></span><br><span class="line"># Hadoop Environment Variables</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop # hadoop的安装路径</span><br><span class="line">export HADOOP_INSTALL=$HADOOP_HOME</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME # 设置MAPRED环境变量</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME # 设置COMMON环境变量</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME # HDFS的环境变量</span><br><span class="line">export YARN_HOME=$HADOOP_HOME # YARN的环境变量</span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_311 # 设置jdk的安装目录（用which java查看到）</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure>

<ul>
<li><p>生效环境变量</p>
<p> <code>source ~/.bashrc</code></p>
</li>
<li><p>修改Hadoop-env.sh中的java_home，不然在启动集群（伪集群）时出现报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line"># 设置如下</span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_311</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><ul>
<li><p>Hadoop 的配置文件位于 <code>/usr/local/hadoop/etc/hadoop/</code> 中，伪分布式需要修改2个配置文件 <strong>core-site.xml</strong> 和 <strong>hdfs-site.xml</strong></p>
</li>
<li><p>Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。</p>
</li>
<li><p>修改配置文件 <strong>core-site.xml</strong> </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /usr/local/hadoop/etc/hadoop/core-site.xml </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"># 设置HDFS的默认名称，在使用命令调用时，可以用此名称</span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>同样的，修改配置文件 <strong>hdfs-site.xml</strong>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>设置blocks副本数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>设置存放NameNode的数据存储目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>设置存放DataNode的数据存储目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置完成后，执行 NameNode 的格式化:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>接着开启 <code>NaneNode</code> 和 <code>DataNode</code> 守护进程，<code>./sbin/start-dfs.sh</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./sbin/start-dfs.sh</span><br><span class="line">Starting namenodes on [localhost]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [VM-24-13-centos]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动完成后，可以通过命令 <code>jps</code> 来判断是否成功启动，若成功启动则会列出如下进程: <code>NameNode</code>、<code>DataNode</code>和<code>SecondaryNameNode</code>（如果 SecondaryNameNode 没有启动，请运行 sbin&#x2F;stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ jps</span><br><span class="line">12433 Jps</span><br><span class="line">11741 DataNode</span><br><span class="line">11597 NameNode</span><br><span class="line">11934 SecondaryNameNode</span><br><span class="line"><span class="comment"># HDFS功能：NameNode，SecondaryNameNode，DataNode已经启动</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>通过查看启动日志分析启动失败原因</strong></p>
<p>有时 Hadoop 无法正确启动，如 NameNode 进程没有顺利启动，这时可以查看启动日志来排查原因，注意几点：</p>
<ul>
<li>启动时会提示形如 “dblab: starting namenode, logging to &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;logs&#x2F;hadoop-hadoop-namenode-dblab.out”，其中 dblab 对应你的主机名，但启动的日志信息是记录在 &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;logs&#x2F;hadoop-hadoop-namenode-dblab.log 中，所以应该查看这个后缀为 <strong>.log</strong> 的文件；</li>
<li>每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，看下记录的时间就知道了。</li>
<li>一般出错的提示在最后面，也就是写着 Fatal、Error 或者 Java Exception 的地方。</li>
<li>可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。</li>
</ul>
</blockquote>
<h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><ul>
<li><p>上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./bin/hdfs dfs -<span class="built_in">mkdir</span> -p /user/hadoop</span><br><span class="line">2021-12-07 16:25:32,492 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure>
</li>
<li><p>接着将·<code>./etc/hadoop</code>中的 xml 文件作为输入文件复制到分布式文件系统中，即将<code> /usr/local/hadoop/etc/hadoop</code> 复制到分布式文件系统中的<code>/user/hadoop/input</code>中。我们使用的是 hadoop 用户，并且已创建相应的用户目录<code> /user/hadoop</code> ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是<code> /user/hadoop/input</code></p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -<span class="built_in">mkdir</span> input</span><br><span class="line">./bin/hdfs dfs -put ./etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure>

<ul>
<li><p>复制完成后，可以通过如下命令查看 HDFS 中的文件列表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./bin/hdfs dfs -<span class="built_in">ls</span> input</span><br><span class="line"></span><br><span class="line">Found 9 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       7861 2021-12-07 16:28 input/capacity-scheduler.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       1071 2021-12-07 16:28 input/core-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup      10206 2021-12-07 16:28 input/hadoop-policy.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       1339 2021-12-07 16:28 input/hdfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        620 2021-12-07 16:28 input/httpfs-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       3518 2021-12-07 16:28 input/kms-acls.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        682 2021-12-07 16:28 input/kms-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        758 2021-12-07 16:28 input/mapred-site.xml</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        690 2021-12-07 16:28 input/yarn-site.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar  wordcount input output </span><br><span class="line">....</span><br><span class="line">2021-12-07 16:30:56,021 INFO mapreduce.Job: Job job_local897809468_0002 completed successfully</span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看运行结果的命令（查看的是位于 HDFS 中的输出结果）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./bin/hdfs dfs -<span class="built_in">ls</span> output/*</span><br><span class="line"></span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2021-12-10 17:58 output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       9405 2021-12-10 17:58 output/part-r-00000</span><br><span class="line"></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ ./bin/hdfs dfs -<span class="built_in">cat</span> output/part-r-00000</span><br><span class="line">....</span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>我们也可以将运行结果取回到本地：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -r ./output    <span class="comment"># 先删除本地的 output 文件夹（如果存在）</span></span><br><span class="line">./bin/hdfs dfs -get output ./output     <span class="comment"># 将 HDFS 上的 output 文件夹拷贝到本机</span></span><br><span class="line">[hadoop@VM-24-13-centos hadoop]$ <span class="built_in">cat</span> ./output/*</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>Hadoop 运行程序时，输出目录不能存在，否则会提示错误 <code>org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists</code>，因此若要再次执行，需要执行如下命令删除 output 文件夹:</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -rm -r output    # 删除 output 文件夹</span><br></pre></td></tr></table></figure>

<ul>
<li>为防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作，可以采用类似于下面java代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Job</span>(conf);</span><br><span class="line"> </span><br><span class="line"><span class="comment">/* 删除输出目录 */</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">outputPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]);</span><br><span class="line">outputPath.getFileSystem(conf).delete(outputPath, <span class="literal">true</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li><p>若要关闭 Hadoop，则运行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>下次启动 hadoop 时，无需进行 NameNode 的初始化，只需要运行 <code>./sbin/start-dfs.sh</code> 就可以！</p>
</li>
</ul>
<h2 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h2><p>后续打算采用本地三台虚拟机的模式搭建完全分布式。</p>
<p>请注意分布式运行中的这几个结点的区别：</p>
<ul>
<li>从分布式存储的角度来说，集群中的结点由一个NameNode和若干个DataNode组成,另有一个SecondaryNameNode作为NameNode的备份。</li>
<li>从分布式应用的角度来说，集群中的结点由一个JobTracker和若干个TaskTracker组成，JobTracker负责任务的调度，TaskTracker负责并行执行任务。TaskTracker必须运行在DataNode上，这样便于数据的本地计算。JobTracker和NameNode则无须在同一台机器上。一个机器上，既当namenode，又当datanode,或者说 既 是jobtracker,又是tasktracker。没有所谓的在多台机器上进行真正的分布式计算，故称为”伪分布式”。</li>
<li>真正的分布式，由3个及以上的实体机或者虚拟机组件的机群。</li>
<li>来自<a target="_blank" rel="noopener" href="https://www.cnblogs.com/liango/p/7116620.html">这里</a></li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/install-hadoop-in-centos/">Hadoop安装教程_伪分布式配置_CentOS6.4&#x2F;Hadoop2.6.0</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/43cfffe7090d">ubuntn-Hadoop 单台Cluster的安装</a></p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/8ae2490c/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/8ae2490c/" class="post-title-link" itemprop="url">第二章 数据仓的设计与构建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-06 16:32:38" itemprop="dateCreated datePublished" datetime="2021-12-06T16:32:38+08:00">2021-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span id="/aposts/8ae2490c/" class="post-meta-item leancloud_visitors" data-flag-title="第二章 数据仓的设计与构建" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/8ae2490c/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/8ae2490c/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="什么是数据仓"><a href="#什么是数据仓" class="headerlink" title="什么是数据仓"></a>什么是数据仓</h2><ul>
<li><p>是BI（商业智能）、报表和数据挖掘等应用的基础</p>
</li>
<li><p>大量的数据集合，4个特点主要包括：<strong>面向主题的、集成的、相对稳定的、反应历史变化的</strong></p>
</li>
<li><p>数据仓至少需要具备<strong>数据获取、数据存储、数据访问</strong>3个核心功能，这3个功能的实现过程是数据源到最终决策应用的流转过程。下图为数据流转图：</p>
</li>
</ul>
<p><img src="/aposts/8ae2490c/image-20211201171618652.png" alt="image-20211201171618652"></p>
<ul>
<li><p>数据获取和数据存储这两个功能主要由ETL工具支撑。ETL是指从<strong>数据源提前，经过清洗、转换</strong>等过程，并最终存储到目标数据仓库的过程。如下图所示，ETL过程3个步骤</p>
<p><img src="/aposts/8ae2490c/image-20211201172008951.png" alt="image-20211201172008951"></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337994072">为什么要用ETL</a></p>
</li>
</ul>
<h2 id="数据仓库、集市、数据湖、中台区别"><a href="#数据仓库、集市、数据湖、中台区别" class="headerlink" title="数据仓库、集市、数据湖、中台区别"></a>数据仓库、集市、数据湖、中台区别</h2><h3 id="数据集市"><a href="#数据集市" class="headerlink" title="数据集市"></a>数据集市</h3><ul>
<li><p>数据仓库面向企业全局业务，而数据集市面向部门级业务</p>
<p><img src="/aposts/8ae2490c/image-20211206151311207.png" alt="image-20211206151311207"></p>
</li>
</ul>
<h3 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a>数据湖</h3><ul>
<li><p>数据存储结构：数据仓主要存储和处理历史数据的机构化数据，而数据湖能存储结构和非结构化所有格式的数据</p>
</li>
<li><p>数据转换处理：数据仓库需要对源数据进行清洗、转换等预处理，以和定义好的数据模型相吻合；而数据湖是从源数据导入，无数据流失，随去随用，只有在使用的时候对数据转换等处理</p>
</li>
<li><p>数据场景：数据仓通常充当商业智能系统、数据仪表盘等可视化报表服务的数据源角色，支持历史分析；数据湖可以作为数据仓库或数据集市的数据源，更适合进行数据的挖掘、探索和预测，</p>
</li>
</ul>
<h3 id="数据中台"><a href="#数据中台" class="headerlink" title="数据中台"></a>数据中台</h3><ul>
<li>由阿里巴巴提出，就是用大数据技术统一处理数据，然后提供API给外部使用，数据中台保护数据仓库和其他服务器中间件</li>
</ul>
<h2 id="数据仓库的设计"><a href="#数据仓库的设计" class="headerlink" title="数据仓库的设计"></a>数据仓库的设计</h2><h3 id="架构分层设计"><a href="#架构分层设计" class="headerlink" title="架构分层设计"></a><span id="架构分层设计">架构分层设计</span></h3><ul>
<li>数据仓库通常可分为数据接入层、数据明细层、数据汇总层、数据集市层、数据应用层、临时层和公共维度层。其中数据明细层和数据汇总层又合称为数据仓库层。</li>
</ul>
<p><img src="/aposts/8ae2490c/image-20211206154539189.png" alt="image-20211206154539189"></p>
<h4 id="数据接入层ODS"><a href="#数据接入层ODS" class="headerlink" title="数据接入层ODS"></a>数据接入层ODS</h4><ul>
<li>（Operational Data Store，ODS），也称数据贴源层，通常从业务数据库直接导入，为了考虑后续可能需要追溯数据问题，因此对于这一层就不建议做过多的数据清洗工作，原封不动地接入原始数据即可，至于数据的去噪、去重、异常值处理等过程可以放在后面的DWD层来做</li>
</ul>
<h4 id="数据明细层DWD"><a href="#数据明细层DWD" class="headerlink" title="数据明细层DWD"></a>数据明细层DWD</h4><ul>
<li>(Data Warehouse Detail，DWD) ，这层和 ODS 层保持一样的数据结构，只不过在从 ODS 里抽取到 DWD 的时候这个过程叫 ETL，后面我们会再讲 ETL，在抽取时对数据进行清洗加工，提供一定的数据质量保证，提供更干净的数据。</li>
</ul>
<h4 id="数据汇总层DWS"><a href="#数据汇总层DWS" class="headerlink" title="数据汇总层DWS"></a>数据汇总层DWS</h4><ul>
<li>（Data Warehouse Summary, DWS），对各个表进行JOIN操作，产生业务所需要的完整数据。该层主要存放明细事实宽表、聚合试试宽表等。</li>
</ul>
<h4 id="数据集市层DWM"><a href="#数据集市层DWM" class="headerlink" title="数据集市层DWM"></a>数据集市层DWM</h4><ul>
<li>也叫数据中间件，(Data Warehouse Middle,DWM)，该层是在DWD层的数据基础上，对数据做一些轻微的聚合操作，生成一些列的中间结果表，提升公共指标的复用性，减少重复加工的工作。</li>
<li>简答来说，对通用的核心维度进行聚合操作，算出相应的统计指标</li>
<li>从广度来说，它包含了所有的业务数据</li>
</ul>
<h4 id="数据应用层"><a href="#数据应用层" class="headerlink" title="数据应用层"></a>数据应用层</h4><ul>
<li>该层中，数据高度汇总，数据粒度较大，但不一定涵盖所有业务数据，也可能只是数据集市层数据的一个子集。</li>
<li>该层主要是提供给数据产品和数据分析使用的数据，一般会存放在ES、Redis、PostgreSql等系统中供线上系统使用；也可能存放在hive或者Druid中，供数据分析和数据挖掘使用，比如常用的数据报表就是存在这里的</li>
</ul>
<h4 id="临时层TMP"><a href="#临时层TMP" class="headerlink" title="临时层TMP"></a>临时层TMP</h4><ul>
<li>临时存放一些中间数据计算结果</li>
</ul>
<h4 id="公共维度层"><a href="#公共维度层" class="headerlink" title="公共维度层"></a>公共维度层</h4><ul>
<li>主要负责一些一致性维度建设，如地点区域表、时间维度表等，数据仓库的各层均可使用此层</li>
</ul>
<h3 id="数据仓库建模方法"><a href="#数据仓库建模方法" class="headerlink" title="数据仓库建模方法"></a><span id="数据仓库建模方法">数据仓库建模方法</span></h3><ul>
<li>主要有范式建模、维度建模、实体建模</li>
</ul>
<h4 id="范式建模"><a href="#范式建模" class="headerlink" title="范式建模"></a>范式建模</h4><p>是数据仓库逻辑模型设计的基本理论，在数据仓库的模型设计中，一般采用第三范式。一个符合第三范式的关系必须具有以下三个条件:</p>
<ul>
<li>每个属性的值唯一,不具有多义性;</li>
<li>每个非主属性必须完全依赖于整个主键,而非主键的一部分;</li>
<li>每个非主属性不能依赖于其他关系中的属性,因为这样的话,这种属性应该归到其他关系中去</li>
</ul>
<h4 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h4><ul>
<li><p>是经典的面向分析的数据仓库建模方法。对数据进行分析时使用的度量。例如：抽取近10年的信用卡数据，分析年申请趋势</p>
</li>
<li><p>经常出现实体表、维度表和事实表等</p>
<ul>
<li>实体表，用于存放商品的属性信息</li>
<li>维度表，按照某个分析维度来组织的事实描述，如分析某商品近半年来每月下单量，则表中一定存在时间字段属性。</li>
<li>事实表，<strong>是维度表各个维度的交点</strong>，如某商品在某地某月的销售额</li>
</ul>
</li>
<li><p>在维度建模的基础上又分为三种模型：星型模型、雪花模型、星座模型。</p>
</li>
</ul>
<h5 id="星型模式"><a href="#星型模式" class="headerlink" title="星型模式"></a>星型模式</h5><p><img src="/aposts/8ae2490c/image-20211206163244801.png" alt="image-20211206163244801"></p>
<h5 id="雪花模式"><a href="#雪花模式" class="headerlink" title="雪花模式"></a>雪花模式</h5><p><img src="/aposts/8ae2490c/image-20211206163338893.png" alt="image-20211206163338893"></p>
<ul>
<li>星型模型和雪花模型实例</li>
</ul>
<p><img src="/aposts/8ae2490c/image-20211206163931436.png" alt="image-20211206163931436"></p>
<h5 id="星座模式"><a href="#星座模式" class="headerlink" title="星座模式"></a>星座模式</h5><p>星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维度信息。</p>
<p><img src="/aposts/8ae2490c/image-20211206163425140.png" alt="image-20211206163425140"></p>
<ul>
<li>星座模型与前两种情况的区别是事实表的数量,星座模型是基于多个事实表。</li>
<li>基本上是很多数据仓库的常态,因为很多数据仓库都是多个事实表的。所以星座不星座只反映是否有多个事实表,他们之间<br>是否共享一些维度表。所以星座模型并不和前两个模型冲突。</li>
</ul>
<h5 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h5><ul>
<li>首先就是星座不星座这个只跟数据和需求有关系,跟殳计没关系,不用选择。星型还是雪花,取决于性能优先,还是灵活更优先。</li>
<li>目前实际企业开发中,不会绝对选择一种,根据情况灵活组合,甚至并存(一层维度和多层维度都保存)</li>
<li>但是整体来看,更倾向于维度更少的星型模型。尤其是hadoop体系,减少Join就是减少 Shuffle,性能差距很大。(关系型数据可以依靠强大的主键索引)</li>
</ul>
<h4 id="实体建模"><a href="#实体建模" class="headerlink" title="实体建模"></a>实体建模</h4><p>在数据仓建模中不常见，一般适用与业务建模和领域概念建模阶段</p>
<h2 id="数据仓库构建"><a href="#数据仓库构建" class="headerlink" title="数据仓库构建"></a>数据仓库构建</h2><h3 id="数据仓库的构建方法"><a href="#数据仓库的构建方法" class="headerlink" title="数据仓库的构建方法"></a>数据仓库的构建方法</h3><ul>
<li>构建方法主要包括自顶向下和自底向上</li>
</ul>
<h4 id="自顶向下实现"><a href="#自顶向下实现" class="headerlink" title="自顶向下实现"></a>自顶向下实现</h4><ul>
<li>自顶向下的实现需要在项目开始时完成更多计划和设计工作，这就需要涉及参与数据仓库实现的每个工作组、部门或业务线中的人员。要使 用的数据源、安全性、数据结构、数据质量、数据标准和整个数据模型的有关决策一般需要在真正的实现开始之前就完成。</li>
<li>此构建方法，实施周期长，难道略大</li>
</ul>
<h4 id="自底向上实现"><a href="#自底向上实现" class="headerlink" title="自底向上实现"></a>自底向上实现</h4><ul>
<li>自底向上的实现包含数据仓库的规划和设计，无需等待安置好更大业务范围的数据仓库设计。这并不意味着不会开发更大业务范围的数据仓 库设计；随着初始数据仓库实现的扩展，将逐渐增加对它的构建。现在，该方法得到了比自顶向下方法更广泛的接受，因为数据仓库的直接结果可以实现， 并可以用作扩展更大业务范围实现的证明。</li>
</ul>
<h4 id="两者结合"><a href="#两者结合" class="headerlink" title="两者结合"></a>两者结合</h4><ul>
<li>两者结合的折中实现：每种实现方法都有利弊。在许多情况下，最好的方法可能是某两种的组合。该方法的关键之一就是确定业务范围的架构需要用于支持 集成的计划和设计的程度，因为数据仓库是用自底向上的方法进行构建。在使用自底向上或阶段性数据仓库项目模型来构建业务范围架构中的一系列数据集 市时，您可以一个接一个地集成不同业务主题领域中的数据集市，从而形成设计良好的业务数据仓库。这样的方法可以极好地适用于业务。在这种方法中， 可以把数据集市理解为整个数据仓库系统的逻辑子集，换句话说数据仓库就是一致化了的数据集市的集合。</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>无论采用哪种模式，数据仓库构建过程，都可以参考下图介绍的5个步骤。基于BI报表、数据挖掘等应用要求，可参考<a href="#%E6%9E%B6%E6%9E%84%E5%88%86%E5%B1%82%E8%AE%BE%E8%AE%A1">架构分层设计</a>数据仓库结构进行适当的分层设计，并根据业务要求选择合适的建模方法，可参考<a href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95">数据仓库建模方法</a></p>
<p><img src="/aposts/8ae2490c/image-20211206171202539.png" alt="image-20211206171202539"></p>
<h3 id="数据仓库实例"><a href="#数据仓库实例" class="headerlink" title="数据仓库实例"></a>数据仓库实例</h3><ul>
<li><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/959/">大数据案例-步骤一:本地数据集上传到数据仓库Hive</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/ae5c089/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/ae5c089/" class="post-title-link" itemprop="url">pandas处理excel</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-03 17:37:40" itemprop="dateCreated datePublished" datetime="2021-12-03T17:37:40+08:00">2021-12-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          
            <span id="/aposts/ae5c089/" class="post-meta-item leancloud_visitors" data-flag-title="pandas处理excel" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/ae5c089/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/ae5c089/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><ul>
<li><p>现有一excel如下，需要把日期和金额合并成一行</p>
<p><img src="/aposts/ae5c089/image-20211203173955957.png" alt="image-20211203173955957"></p>
</li>
<li><p>最终需要实现效果如下：</p>
<p><img src="/aposts/ae5c089/image-20211203174051646.png" alt="image-20211203174051646"></p>
</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OperateExcel</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        self.file_path = file_path</span><br><span class="line">        <span class="comment"># 注意header参数，取值为前面两行</span></span><br><span class="line">        self.df = pd.read_excel(file_path, sheet_name=<span class="string">&quot;日记帐&quot;</span>, header=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_head</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        头部处理，格式：</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        [year, headers]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        year为年，如2021年</span></span><br><span class="line"><span class="string">        headers=[]，把所有头部值存到[]中，日期	凭证编号	车牌号	拿货吨位	卖出吨位	摘要	收入	支出	余额</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        headers = []</span><br><span class="line">        <span class="comment"># 取头部数据</span></span><br><span class="line">        head = self.df.head(<span class="number">0</span>)</span><br><span class="line">        year = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> items <span class="keyword">in</span> head:</span><br><span class="line">            <span class="comment"># ------ 处理日期,取年</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> items:</span><br><span class="line">                <span class="keyword">if</span> i.find(<span class="string">&quot;年&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                    year = i</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># ------- 处理其他头部</span></span><br><span class="line">            <span class="keyword">if</span> items[<span class="number">1</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>).find(<span class="string">&quot;Unnamed&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                headers.append(items[<span class="number">0</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">            <span class="comment"># ---- 金额后的头部，只取收入、支出、余额；</span></span><br><span class="line">            <span class="keyword">if</span> items[<span class="number">0</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>).find(<span class="string">&quot;金额&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                headers.append(items[<span class="number">1</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line"></span><br><span class="line">        headers.insert(<span class="number">0</span>, <span class="string">&quot;日期&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> year, headers</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">         处理数据</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">         [file_name, data]</span></span><br><span class="line"><span class="string">         file_name表示将要保持的名字</span></span><br><span class="line"><span class="string">         data 表示处理好的数据，给pd.DataFrame使用，格式 &#123;&quot;金额&quot;:[1,2,3],&quot;收入&quot;:[]&#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        g_head = self.get_head()</span><br><span class="line">        headers = g_head[<span class="number">1</span>]</span><br><span class="line">        year = g_head[<span class="number">0</span>]</span><br><span class="line">        month = <span class="string">&quot;&quot;</span></span><br><span class="line">        data = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> headers:</span><br><span class="line">            data[i] = []</span><br><span class="line">        <span class="keyword">for</span> items <span class="keyword">in</span> self.df.values:</span><br><span class="line">            <span class="comment"># 合计不取数据；第一个单元格为空值说明此行为无效数据，也不取值</span></span><br><span class="line">            <span class="keyword">if</span> items[<span class="number">0</span>] == <span class="string">&#x27;合计&#x27;</span> <span class="keyword">or</span> math.isnan(items[<span class="number">0</span>]):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 处理日期值为：2021年1月1日</span></span><br><span class="line">            month = <span class="built_in">str</span>(items[<span class="number">0</span>]) + <span class="string">&quot;月&quot;</span></span><br><span class="line">            m_date = year + month + <span class="built_in">str</span>(<span class="built_in">int</span>(items[<span class="number">1</span>])) + <span class="string">&quot;日&quot;</span></span><br><span class="line">            data[<span class="string">&quot;日期&quot;</span>].append(m_date)</span><br><span class="line">            <span class="comment"># 凭证编号</span></span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">2</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                num = items[<span class="number">2</span>]</span><br><span class="line">            data[<span class="string">&quot;凭证编号&quot;</span>].append(num)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 车牌号</span></span><br><span class="line">            car_num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">3</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                car_num = items[<span class="number">3</span>]</span><br><span class="line">            data[<span class="string">&quot;车牌号&quot;</span>].append(car_num)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 拿货吨位</span></span><br><span class="line">            take_tonnage = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">4</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                take_tonnage = items[<span class="number">4</span>]</span><br><span class="line">            data[<span class="string">&quot;拿货吨位&quot;</span>].append(take_tonnage)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 卖出吨位</span></span><br><span class="line">            sell_tonnage = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">5</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                sell_tonnage = items[<span class="number">5</span>]</span><br><span class="line">            data[<span class="string">&quot;卖出吨位&quot;</span>].append(sell_tonnage)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 摘要</span></span><br><span class="line">            summary = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">6</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                summary = items[<span class="number">6</span>]</span><br><span class="line">            data[<span class="string">&quot;摘要&quot;</span>].append(summary)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 收入</span></span><br><span class="line">            revenue = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">7</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                revenue = items[<span class="number">7</span>]</span><br><span class="line">            data[<span class="string">&quot;收入&quot;</span>].append(revenue)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 支出</span></span><br><span class="line">            spend = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">8</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                spend = items[<span class="number">8</span>]</span><br><span class="line">            data[<span class="string">&quot;支出&quot;</span>].append(spend)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 余额</span></span><br><span class="line">            balance = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(items[<span class="number">9</span>]) != <span class="string">&quot;nan&quot;</span>:</span><br><span class="line">                balance = items[<span class="number">9</span>]</span><br><span class="line">            data[<span class="string">&quot;余额&quot;</span>].append(balance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成的文件名</span></span><br><span class="line">        file_name = year + month + <span class="string">&quot;.xlsx&quot;</span></span><br><span class="line">        <span class="keyword">return</span> file_name, data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sum_data</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">       新增统计处理的数据，最终给pd.DataFrame使用</span></span><br><span class="line"><span class="string">       :param data: list|表示处理好的数据，给pd.DataFrame使用，格式 &#123;&quot;金额&quot;:[1,2,3],&quot;收入&quot;:[]&#125;</span></span><br><span class="line"><span class="string">       :return: list</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">            <span class="comment"># 插入一行空行</span></span><br><span class="line">            data[i].append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 手动合计拿货吨位、卖出吨位、收入、支出、余额</span></span><br><span class="line">        data[<span class="string">&quot;日期&quot;</span>].append(<span class="string">&quot;合计&quot;</span>)</span><br><span class="line">        <span class="comment"># data[&quot;拿货吨位&quot;].append(sum(map(float,data[&quot;拿货吨位&quot;])))</span></span><br><span class="line">        data[<span class="string">&quot;拿货吨位&quot;</span>].append(<span class="built_in">sum</span>(data[<span class="string">&quot;拿货吨位&quot;</span>]))</span><br><span class="line">        data[<span class="string">&quot;卖出吨位&quot;</span>].append(<span class="built_in">sum</span>(data[<span class="string">&quot;卖出吨位&quot;</span>]))</span><br><span class="line">        data[<span class="string">&quot;收入&quot;</span>].append(<span class="built_in">sum</span>(data[<span class="string">&quot;收入&quot;</span>]))</span><br><span class="line">        data[<span class="string">&quot;支出&quot;</span>].append(<span class="built_in">sum</span>(data[<span class="string">&quot;支出&quot;</span>]))</span><br><span class="line">        data[<span class="string">&quot;余额&quot;</span>].append(<span class="built_in">sum</span>(data[<span class="string">&quot;余额&quot;</span>]))</span><br><span class="line">        <span class="comment"># 对手动合计拿货吨位、卖出吨位、收入、支出、余额、日期之外的合计行，写入空值</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;日期&quot;</span>, <span class="string">&quot;拿货吨位&quot;</span>, <span class="string">&quot;卖出吨位&quot;</span>, <span class="string">&quot;收入&quot;</span>, <span class="string">&quot;支出&quot;</span>, <span class="string">&quot;余额&quot;</span>]:</span><br><span class="line">                data[i].append(<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果数据为0，填入到excel中不美观，因此改为”“，默认填进去就是不可见</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data[j])):</span><br><span class="line">                <span class="keyword">if</span> data[j][k] == <span class="number">0</span>:</span><br><span class="line">                    data[j][k] = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_excel</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重新生成excel</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        g_data = self.get_data()</span><br><span class="line">        <span class="comment"># 得到要保存的文件名</span></span><br><span class="line">        file_name = g_data[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 得到处理好的数据，不包括统计</span></span><br><span class="line">        l_data = g_data[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 得到处理好的数据，包括统计</span></span><br><span class="line">        s_data = self.sum_data(l_data)</span><br><span class="line">        info_marks = pd.DataFrame(s_data)</span><br><span class="line">        writer = pd.ExcelWriter(file_name)</span><br><span class="line">        info_marks.to_excel(writer, index=<span class="literal">False</span>)</span><br><span class="line">        writer.save()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;生成excel成功，文件名为：%s&#x27;</span> % file_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    file_path = <span class="string">&#x27;XXXX.xls&#x27;</span></span><br><span class="line">    o_excel = OperateExcel(file_path)</span><br><span class="line">    o_excel.generate_excel()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>取头部要取两行</li>
<li>注意nan的处理</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/f05ef477/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/f05ef477/" class="post-title-link" itemprop="url">第一章 大数据技术生态</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-01 09:35:36" itemprop="dateCreated datePublished" datetime="2021-12-01T09:35:36+08:00">2021-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span id="/aposts/f05ef477/" class="post-meta-item leancloud_visitors" data-flag-title="第一章 大数据技术生态" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/f05ef477/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/f05ef477/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="大数据技术生态"><a href="#大数据技术生态" class="headerlink" title="大数据技术生态"></a>大数据技术生态</h1><blockquote>
<p>本文主要抄录《大数据测试技术与实践》</p>
</blockquote>
<p>由下而上可以划分为：</p>
<ul>
<li>数据采集<ul>
<li>关系与非关系数据采集组件，分布式消息队列等，如kafka、sqoop</li>
</ul>
</li>
<li>数据存储<ul>
<li>分布式存储系统、关系和非关系数据库等，如HDFS、MySQL</li>
</ul>
</li>
<li>管理调度<ul>
<li>资源管理和调度YARN，容器Kubernetes、服务协调zookeeper、工作流调度平台（如Azkaban）等</li>
</ul>
</li>
<li>计算机分析<ul>
<li>批处理（MapReduce）、流计算(Flink)、查询分析(Impala)和图计算(Gelly)等</li>
</ul>
</li>
<li>组件应用<ul>
<li>各种数据分析和机器学习工具，如Hive、Pig、TensorFlow</li>
</ul>
</li>
</ul>
<h2 id="大数据采集计算"><a href="#大数据采集计算" class="headerlink" title="大数据采集计算"></a>大数据采集计算</h2><ul>
<li>系统日志采集，如kafka、Flume</li>
<li>网络数据采集，如爬虫</li>
<li>其他数据采集</li>
</ul>
<h2 id="大数据存储计算"><a href="#大数据存储计算" class="headerlink" title="大数据存储计算"></a>大数据存储计算</h2><h3 id="分布式文件系统-HDFS"><a href="#分布式文件系统-HDFS" class="headerlink" title="分布式文件系统 HDFS"></a>分布式文件系统 HDFS</h3><ul>
<li><p>主要解决大数据存储问题</p>
</li>
<li><p>GFS（Google File System）的开源实现</p>
</li>
<li><p>Hadoop两大核心组成部分之一，另外一个是MapReduce</p>
</li>
<li><p>遵循主从（master&#x2F;salve）框架</p>
</li>
<li><p>可以由单台服务器扩展到数千台服务器</p>
</li>
<li><p>NameNode关联和维护HDFS文件系统的读写操作</p>
</li>
<li><p>多个DataNode1负责存储数据</p>
</li>
</ul>
<p><img src="/aposts/f05ef477/image-20211201095407758.png" alt="image-20211201095407758"></p>
<h4 id="HDFS优点"><a href="#HDFS优点" class="headerlink" title="HDFS优点"></a>HDFS优点</h4><ul>
<li>具有高度容错能力，能实时监测错误并且自动恢复。<ul>
<li>类似于服务器的容灾能力，当某台服务器挂了，就启用备用服务器，进行数据同步</li>
</ul>
</li>
<li>数据存储为Streaming流式数据存储 。批处理数据，而不是实时处理，提高了大量处理数据的能力，但是会<strong>牺牲响应时间</strong></li>
<li>大数据集。提供了<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=%5Bhttps://www.ibm.com/developerworks/cn/linux/cluster/lw-clustering.html%5D(https://www.ibm.com/developerworks/cn/linux/cluster/lw-clustering.html)">cluster</a>集群架构，集群可扩展为数百个节点</li>
<li>数据简单一致性。一次性写入多次读取，一个文件创建后就不可再修改，这样可以简化数据一致性</li>
<li>跨硬件和跨软件。平台的可移植性</li>
</ul>
<h3 id="海量数据列式存储：Hbase"><a href="#海量数据列式存储：Hbase" class="headerlink" title="海量数据列式存储：Hbase"></a>海量数据列式存储：Hbase</h3><ul>
<li><p>HDFS容错率很高，即便是在系统崩溃的情况下，也能够在节点之间快速传输数据。<strong>HBase是非关系数据库，是开源的Not-Only-SQL数据库</strong>，它的运行<strong>建立在Hadoop上</strong>。HBase依赖于CAP定理(Consistency, Availability, and Partition Tolerance）中的CP项</p>
</li>
<li><p>HDFS最适于执行批次分析。然而，<strong>它最大的缺点是无法执行实时分析</strong>，而实时分析是信息科技行业的标配。<strong>HBase能够处理大规模数据，它不适于批次分析，但它可以向Hadoop实时地调用数据</strong>。</p>
</li>
<li><p>HDFS和HBase都可以处理结构、半结构和非结构数据。因为HDFS建立在旧的MapReduce框架上，所以它缺乏内存引擎，数据分析速度较慢。相反，HBase使用了内存引擎，大大提高了数据的读写速度。</p>
</li>
<li><p>HDFS执行的数据分析过程是透明的。HBase与之相反，因为其结构基于NoSQL，它通过在不同的关键字下进行排序而获取数据。</p>
</li>
</ul>
<p><img src="/aposts/f05ef477/image-20211201100832525.png" alt="image-20211201100832525"></p>
<h2 id="大数据分析技术"><a href="#大数据分析技术" class="headerlink" title="大数据分析技术"></a>大数据分析技术</h2><ul>
<li>批处理计算。针对大规模数据的批量处理，主要代表产品有MapReduce、Spark</li>
<li>流计算。针对流数据的实时计算，主要代表产品有，Spark Streaming、Flink 、Storm等</li>
<li>查询分析计算。针对大规模数据的存储管理和查询分析，主要代表产品有Hive、Impala等</li>
<li>图计算。针对大规模图结构数据的处理，主要代表产品有Pregel、Gelly等</li>
</ul>
<h3 id="批处理计算-MapReduce"><a href="#批处理计算-MapReduce" class="headerlink" title="批处理计算-MapReduce"></a>批处理计算-MapReduce</h3><ul>
<li><p>进行大量数据处理时，用MapReduce进行分布式计算，这样可大量减少计算时间，还有Spark、Pig等就是类似的代表产品或技术</p>
</li>
<li><p>Map将任务分割成更小任务，由每台服务器分别执行</p>
</li>
<li><p>Reduce将所有服务器返回的结果汇总，整理成最终结果</p>
</li>
</ul>
<p><img src="/aposts/f05ef477/image-20211201102027603.png" alt="image-20211201102027603"></p>
<h3 id="流计算"><a href="#流计算" class="headerlink" title="流计算"></a>流计算</h3><ul>
<li><p>流式处理假设数据的潜在价值是数据的<strong>新鲜度、实时性</strong>，需要尽快处理得到结果。在这种方式下，数据以流的方式到达。在数据连续到达的过程中，由于流携带了大量数据，只有小部分的流数据被保存在有限的<strong>内存中</strong>。流处理方式用于在线应用，通常工作在<strong>秒或毫秒级</strong>别。</p>
</li>
<li><p>目前主流的流处理组件包括：<strong>Strom</strong>、<strong>Spark Streaming</strong>、KafKa、Flume、<strong>Flink</strong>、S3等</p>
<ul>
<li>Spark 和 Strom、Flink对比，无法在对实施要求很高的流处理场景中</li>
</ul>
</li>
</ul>
<p><img src="/aposts/f05ef477/image-20211201104503379.png" alt="image-20211201104503379"></p>
<h3 id="OLAP引擎"><a href="#OLAP引擎" class="headerlink" title="OLAP引擎"></a>OLAP引擎</h3><ul>
<li>即联机分析处理。OLAP对业务数据执行多维分析，并提供复杂计算，趋势分析和复杂数据建模的能力。它主要用于支持企业决策管理分析，是许多商务智能（BI）应用程序背后的技术。</li>
<li>目前开源的引擎很多，如Hive、Impala、Persto等</li>
</ul>
<h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><p><img src="/aposts/f05ef477/image-20211201111306662.png" alt="image-20211201111306662"></p>
<h4 id="其他参考"><a href="#其他参考" class="headerlink" title="其他参考"></a>其他参考</h4><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43665254/article/details/112552106">Hive SQL vs SQL 区别</a></li>
</ul>
<h2 id="大数据管理调度技术"><a href="#大数据管理调度技术" class="headerlink" title="大数据管理调度技术"></a>大数据管理调度技术</h2><h3 id="分布式集群资源调度框架-YARN"><a href="#分布式集群资源调度框架-YARN" class="headerlink" title="分布式集群资源调度框架-YARN"></a>分布式集群资源调度框架-YARN</h3><ul>
<li>针对Hadoop1.0中MR的不足，引入了Yarn框架。Yarn框架中将JobTracker资源分配和作业控制分开，分为Resource Manager(RM)以及Application Master(AM)。</li>
<li>Hadoop的MapReduce架构称为<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=%5Bhttp://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html%5D(http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)">YARN</a>(另一种资源协助者)，是效率更高的资源管理器核心</li>
<li>Client客户端，用户向Resource Manage请求执行运算</li>
<li>在NameNode会有Resource Manage统筹管理运算请求</li>
<li>在其他的DateNode会有 Node Manager负责运行，监督每个任务运行情况，并向Resource Manage 汇报状态</li>
</ul>
<p><img src="/aposts/f05ef477/image-20211201102156648.png" alt="image-20211201102156648"></p>
<h3 id="容器管理系统：Kubernetes"><a href="#容器管理系统：Kubernetes" class="headerlink" title="容器管理系统：Kubernetes"></a>容器管理系统：Kubernetes</h3><ul>
<li>常见的大数据技术组件一般有对应的开源项目支持部署，如Flink，Spark也有官方支持Spark on Kubernetes运行模式等</li>
</ul>
<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><ul>
<li><p>是大数据的动物管理员，是一个开源的分布式的，是Hadoop的一个子项目</p>
</li>
<li><p>Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应，从而实现集群中类似Master&#x2F;Slave管理模式</p>
</li>
<li><p><strong>Zookeeper&#x3D;文件系统+通知机制</strong></p>
</li>
</ul>
<h3 id="常用的工作流调度平台"><a href="#常用的工作流调度平台" class="headerlink" title="常用的工作流调度平台"></a>常用的工作流调度平台</h3><ul>
<li>对于简单的任务调度，可以使用Linux Crontab，但是对于在多台机器上、任务之前有依赖关系时，Linux Crontab就不能满足需求，因此需要分布式任务调度系统来进行任务编排</li>
<li>业界常用开源调度平台有：Azkaban、Oozie、Airflow等</li>
</ul>
<h2 id="大数据商业产品"><a href="#大数据商业产品" class="headerlink" title="大数据商业产品"></a>大数据商业产品</h2><ul>
<li>大数据解决方案提供商</li>
<li>大数据云计算服务商</li>
<li>大数据SaaS服务商</li>
<li>大数据开发平台</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/66ac71c6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/66ac71c6/" class="post-title-link" itemprop="url">寻找拐点</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-24 11:42:59" itemprop="dateCreated datePublished" datetime="2021-11-24T11:42:59+08:00">2021-11-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/jmeter/" itemprop="url" rel="index"><span itemprop="name">jmeter</span></a>
                </span>
            </span>

          
            <span id="/aposts/66ac71c6/" class="post-meta-item leancloud_visitors" data-flag-title="寻找拐点" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/66ac71c6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/66ac71c6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul>
<li>jmeter 5.4.1</li>
<li>测试论坛登录并发的拐点</li>
</ul>
<h2 id="第一次线程组脚本配置"><a href="#第一次线程组脚本配置" class="headerlink" title="第一次线程组脚本配置"></a>第一次线程组脚本配置</h2><ul>
<li><p>设置并发10个请求，永远</p>
<p><img src="/aposts/66ac71c6/image-20211126145852042.png" alt="image-20211126145852042"></p>
</li>
<li><p>设置RPS定时器，将RPS逐渐增加到50&#x2F;S，并持续一段时间</p>
</li>
</ul>
<p><img src="/aposts/66ac71c6/image-20211126150023226.png" alt="image-20211126150023226"></p>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><ul>
<li><p>首先分析Hits per Second，在54秒后，RPS为84.7&#x2F;S（可以理解为：<strong>最大支持1秒内84.7个用户同时登录</strong>），<strong>出现拐点</strong>，请求曲线开始收窄，有同学会问，怎么会是RPS？不是HPS吗？因为在单接口请求下，我么可以认为HPS和RPS是相等的</p>
<p><img src="/aposts/66ac71c6/image-20211129113816875.png" alt="image-20211129113816875"></p>
</li>
<li><p>查看聚合报告，得到RT的响应时间，平均值为203，，这时候可能会有同学说，你这里不能取平均时间，不具有代表性，我们还应该考虑90%,95%，99%不同比例的接口响应时间。对的，个人认为，这些数值在和平均值没有过大的区别的情况下，我们可以取平均值来计算，如果说出现了较大的波动，那么我们需要考虑是不是服务器的内存,cpu出现了问题。</p>
</li>
<li><p>这里我们算下并发数：<code>84.7*0.203=17.19</code>，可以理解为，<strong>支持17.19个用户在1s内同时登录</strong></p>
</li>
<li><p>查看TPS ，发现在16秒支持<strong>最大tps为12</strong>，不然就会出现明显波动</p>
<p><img src="/aposts/66ac71c6/image-20211129114833158.png" alt="image-20211129114833158"></p>
</li>
</ul>
<h3 id="第一次运行结果总结"><a href="#第一次运行结果总结" class="headerlink" title="第一次运行结果总结"></a>第一次运行结果总结</h3><ul>
<li>我们的<strong>最大请求数为84</strong></li>
<li><strong>最大 TPS 为12</strong></li>
<li><strong>最大系统并发数在 17左右</strong></li>
<li>超出这些范围就开始出现波动</li>
</ul>
<h2 id="第二次调整运行参数"><a href="#第二次调整运行参数" class="headerlink" title="第二次调整运行参数"></a>第二次调整运行参数</h2><ul>
<li><p>并发数调整为<strong>5个</strong></p>
<p><img src="/aposts/66ac71c6/image-20211129145513074.png" alt="image-20211129145513074"></p>
<h3 id="运行结果分析"><a href="#运行结果分析" class="headerlink" title="运行结果分析"></a>运行结果分析</h3></li>
<li><p>在54秒时，RPS的值为<strong>84.3&#x2F;S</strong></p>
<p><img src="/aposts/66ac71c6/image-20211129145631570.png" alt="image-20211129145631570"></p>
</li>
<li><p>查看聚合报告，得到RT的响应时间，平均值为175，这里我们算下<strong>并发数：84.3*0.175&#x3D;14.17</strong></p>
</li>
</ul>
<p><img src="/aposts/66ac71c6/image-20211129145944164.png" alt="image-20211129145944164"></p>
<ul>
<li>查看TPS，在21秒是，<strong>tps的值为18</strong>为最佳，不然后续出现巨大波动</li>
</ul>
<p><img src="/aposts/66ac71c6/image-20211129150633788.png" alt="image-20211129150633788"></p>
<h3 id="第二次结果分析"><a href="#第二次结果分析" class="headerlink" title="第二次结果分析"></a>第二次结果分析</h3><ul>
<li>我们的<strong>最大请求数为84</strong></li>
<li><strong>最大 TPS 为18</strong></li>
<li><strong>最大系统并发数在 14左右</strong></li>
<li>超出这些范围就开始出现波动</li>
</ul>
<h2 id="CLI运行"><a href="#CLI运行" class="headerlink" title="CLI运行"></a>CLI运行</h2><ul>
<li>10个并发</li>
</ul>
<h3 id="命令运行"><a href="#命令运行" class="headerlink" title="命令运行"></a>命令运行</h3><ul>
<li>发起压测请求<code>jmeter -n -t resp.jmx -l result/report.jtl</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">E:\jmeter&gt;jmeter -n -t resp.jmx -l result/report.jtl</span><br><span class="line">Creating summariser &lt;summary&gt;</span><br><span class="line">Created the tree successfully using resp.jmx</span><br><span class="line">Starting standalone <span class="built_in">test</span> @ Tue Nov 30 10:06:56 CST 2021 (1638238016471)</span><br><span class="line">Waiting <span class="keyword">for</span> possible Shutdown/StopTestNow/HeapDump/ThreadDump message on port 4445</span><br><span class="line">summary +      9 <span class="keyword">in</span> 00:00:03 =    2.6/s Avg:   140 Min:   117 Max:   230 Err:     0 (0.00%) Active: 5 Started: 5 Finished: 0</span><br><span class="line">summary +    406 <span class="keyword">in</span> 00:00:30 =   13.7/s Avg:   148 Min:   108 Max:  1160 Err:     0 (0.00%) Active: 5 Started: 5 Finished: 0</span><br><span class="line">summary =    415 <span class="keyword">in</span> 00:00:33 =   12.5/s Avg:   148 Min:   108 Max:  1160 Err:     0 (0.00%)</span><br><span class="line">summary +    516 <span class="keyword">in</span> 00:00:30 =   17.2/s Avg:   232 Min:   108 Max: 15728 Err:     0 (0.00%) Active: 5 Started: 5 Finished: 0</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>

<ul>
<li><p>压测完毕后，转变为测试报告</p>
<p><code>jmeter -g report.jtl -o report</code></p>
<p><img src="/aposts/66ac71c6/image-20211130102551759.png" alt="image-20211130102551759"></p>
<h3 id="报告分析"><a href="#报告分析" class="headerlink" title="报告分析"></a>报告分析</h3></li>
<li><p>RPS为<strong>80.48&#x2F;S</strong></p>
<p><img src="/aposts/66ac71c6/image-20211130102819087.png" alt="image-20211130102819087"></p>
</li>
<li><p>得到RT的响应时间，平均值为121.14，这里我们算下<strong>并发数：80.4*0.121&#x3D;14.17</strong></p>
</li>
</ul>
<p><img src="/aposts/66ac71c6/image-20211130103151709.png" alt="image-20211130103151709"></p>
<ul>
<li><p><strong>tps的值为27</strong>为最佳，不然后续出现巨大波动</p>
<p><img src="/aposts/66ac71c6/image-20211130104030732.png" alt="image-20211130104030732"></p>
</li>
</ul>
<h3 id="第三次结果分析"><a href="#第三次结果分析" class="headerlink" title="第三次结果分析"></a>第三次结果分析</h3><ul>
<li>我们的<strong>最大请求数为84</strong></li>
<li><strong>最大 TPS 为27</strong></li>
<li><strong>最大系统并发数在 14左右</strong></li>
<li>超出这些范围就开始出现波动</li>
</ul>
<h2 id="三次结果总结"><a href="#三次结果总结" class="headerlink" title="三次结果总结"></a>三次结果总结</h2><ul>
<li>我们的<strong>最大请求数为84</strong></li>
<li><strong>最大 TPS 为12-27</strong><ul>
<li>CLI模式下TPS支持最大</li>
</ul>
</li>
<li><strong>最大系统并发数在 14-17</strong></li>
<li>这些范围就开始出现波动</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>可以测试多次，取平均值</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/limenglovechen/article/details/107685933">参考这里</a>的测试方式</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/cb7ef82f/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/cb7ef82f/" class="post-title-link" itemprop="url">Concurrency Thread Group实例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-09 15:08:46" itemprop="dateCreated datePublished" datetime="2021-11-09T15:08:46+08:00">2021-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/jmeter/" itemprop="url" rel="index"><span itemprop="name">jmeter</span></a>
                </span>
            </span>

          
            <span id="/aposts/cb7ef82f/" class="post-meta-item leancloud_visitors" data-flag-title="Concurrency Thread Group实例" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/cb7ef82f/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/cb7ef82f/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Concurrency-Thread-Group的介绍"><a href="#Concurrency-Thread-Group的介绍" class="headerlink" title="Concurrency Thread Group的介绍"></a>Concurrency Thread Group的介绍</h2><ul>
<li>Concurrency Thread Group提供了用于配置多个线程计划的简化方法</li>
<li>该线程组目的是为了保持并发水平，意味着如果并发线程不够，则在运行线程中启动额外的线程</li>
<li>和Standard Thread Group不同，它不会预先创建所有线程，因此不会使用额外的内存</li>
<li>对于上篇讲到的Stepping Thread Group来说，Concurrency Thread Group是个更好的选择，因为它允许线程优雅地完成其工作</li>
<li>Concurrency Thread Group提供了更好的用户行为模拟，因为它使您可以更轻松地控制测试的时间，并创建替换线程以防线程在过程中完成</li>
</ul>
<h2 id="Concurrency-Thread-Group参数讲解"><a href="#Concurrency-Thread-Group参数讲解" class="headerlink" title="Concurrency Thread Group参数讲解"></a>Concurrency Thread Group参数讲解</h2><p><img src="/aposts/cb7ef82f/image-20211109151532330.png" alt="image-20211109151532330"></p>
<ul>
<li><strong>Target Concurrency</strong>：目标并发（线程数），我设置为40</li>
<li><strong>Ramp Up Time</strong>：启动时间；若设置 1 min，则目标线程在1 imn内全部启动</li>
<li><strong>Ramp-Up Steps Count</strong>：阶梯次数；若设置 5 ，则目标线程在 1min 内分5次阶梯加压（启动线程）；<strong>每次启动的线程数</strong> &#x3D; 目标线程数 &#x2F; 阶梯次数 &#x3D; 40 &#x2F; 5 &#x3D; 8</li>
<li><strong>Hold Target Rate Time</strong>：持续负载运行时间；若设置 2 ，则启动完所有线程后，持续负载运行 2 min，然后再结束</li>
<li><strong>Time Unit</strong>：时间单位（分钟或者秒）</li>
<li><strong>Thread Iterations Limit：</strong>线程迭代次数限制（循环次数）；默认为空，理解成永远，如果<strong>运行时间到达</strong>Ramp Up Time + Hold Target Rate Time，则停止运行线程<strong>【不建议设置该值】</strong></li>
<li><strong>Log Threads Status into File：</strong>将线程状态记录到文件中（将线程启动和线程停止事件保存为日志文件）；</li>
</ul>
<h4 id="特别注意点"><a href="#特别注意点" class="headerlink" title="特别注意点"></a>特别注意点</h4><ul>
<li>Target Concurrency只是个<strong>期望值</strong>，实际不一定可以达到这个并发数，得看上面的配置<strong>【电脑性能、网络、内存、CPU等因素都会影响最终并发线程数】</strong></li>
<li>Jmeter会根据Target Concurrency的值和当前处于<strong>活动状态的线程数</strong>来判断当前并发线程数是否达到了Target Concurrency；若没有，则会不断启动线程，尽力让并发线程数达到Target Concurrency的值</li>
</ul>
<h2 id="Concurrency-Thread-Group和Stepping-Thread-Group的区别"><a href="#Concurrency-Thread-Group和Stepping-Thread-Group的区别" class="headerlink" title="Concurrency Thread Group和Stepping Thread Group的区别"></a>Concurrency Thread Group和Stepping Thread Group的区别</h2><h4 id="官方说法"><a href="#官方说法" class="headerlink" title="官方说法"></a>官方说法</h4><ul>
<li>Stepping Thread Group不提供设置启动延迟时间，阶梯增压过渡时间，阶梯释放过渡时间，但Concurrency Thread Group提供</li>
<li>Stepping Thread Group可以<strong>阶梯释放线程</strong>，而Concurrency Thread Group是<strong>瞬时释放</strong>（具体看下面介绍）</li>
<li>Stepping Thread Group设置了需要启动多少个线程就会<strong>严格执行</strong>，Concurrency Thread Group会<strong>尽力启动线程达到</strong>Target Concurrency值</li>
</ul>
<h4 id="通俗点理解"><a href="#通俗点理解" class="headerlink" title="通俗点理解"></a>通俗点理解</h4><ul>
<li>Stepping Thread Group 是手动场景：测试过程，按照设定好的步骤执行</li>
<li>Concurrency Thread Group 是目标场景：达到某个目标运行场景，测试过程不可控，动态变化</li>
</ul>
<h4 id="类比-LR"><a href="#类比-LR" class="headerlink" title="类比 LR"></a>类比 LR</h4><ul>
<li>Stepping Thread Group ：设置并发用户数，持续时间等，每隔多少时间自动增加多少个用户</li>
<li>Concurrency Thread Group：预设一个目标并发数，每隔一段时间增加一部分并发数，直到 TPS 达到目标并发数，然后持续运行一段时间</li>
</ul>
<h2 id="Concurrency-Thread-Group-Active-Threads-Over-Time"><a href="#Concurrency-Thread-Group-Active-Threads-Over-Time" class="headerlink" title="Concurrency Thread Group + Active Threads Over Time"></a>Concurrency Thread Group + Active Threads Over Time</h2><p><img src="/aposts/cb7ef82f/image-20211109153859390.png" alt="image-20211109153859390"></p>
<h4 id="第一个关注点：阶梯增压过程"><a href="#第一个关注点：阶梯增压过程" class="headerlink" title="第一个关注点：阶梯增压过程"></a>第一个关注点：阶梯增压过程</h4><p>看Concurrency Thread Group负载预览图每次阶梯增压都是瞬时增压的，但是实际测试结果可以看到它也是有一个过渡期，并不是瞬时增压</p>
<h4 id="第二个关注点：持续负载运行结束后，所有线程瞬时释放"><a href="#第二个关注点：持续负载运行结束后，所有线程瞬时释放" class="headerlink" title="第二个关注点：持续负载运行结束后，所有线程瞬时释放"></a>第二个关注点：持续负载运行结束后，所有线程瞬时释放</h4><ul>
<li>从图最后可以看到，所有线程都是瞬时释放的</li>
<li>普通的线程组有三种状态：启动、运行、释放；而Concurrency Thread Group的线程可以理解成只有两种状态：启动、运行；因为线程都在极短的时间内就结束了</li>
</ul>
<h2 id="Concurrency-Thread-Group的扩展"><a href="#Concurrency-Thread-Group的扩展" class="headerlink" title="Concurrency Thread Group的扩展"></a>Concurrency Thread Group的扩展</h2><ul>
<li>当Concurrency Thread Group与Throughput Shaping Timer（吞吐量计时器）一起使用时，可以用tstFeedback 函数的调用来动态维护实现目标RPS所需的线程数</li>
<li>使用此方法时， 需要将Ramp Up Time 和 Ramp-Up Steps Count 置空</li>
<li>但要确保 Hold Target Rate Time ≥ Throughput Shaping Timer 时间表中指定的总持续时间值（Duration）</li>
</ul>
<p><strong>本文来自<a target="_blank" rel="noopener" href="https://www.cnblogs.com/poloyy/p/12845465.html">这里</a>，强烈建议仔细学习此博主的jmeter系列文章</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/d39845c4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/d39845c4/" class="post-title-link" itemprop="url">混合场景Stepping Thread Group</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-03 10:44:25" itemprop="dateCreated datePublished" datetime="2021-11-03T10:44:25+08:00">2021-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/jmeter/" itemprop="url" rel="index"><span itemprop="name">jmeter</span></a>
                </span>
            </span>

          
            <span id="/aposts/d39845c4/" class="post-meta-item leancloud_visitors" data-flag-title="混合场景Stepping Thread Group" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/d39845c4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/d39845c4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul>
<li><p>本次主要用梯次压测模式进行负载测试</p>
</li>
<li><p>有浏览帖子，回复帖子等场景</p>
</li>
<li><p>随着版本的迭代，已经有更好的线程组代替<code>Stepping Thread Group</code>了【<code>Concurrency Thread Group</code>】，所以说<code>Stepping Thread Group</code>已经是过去式了</p>
</li>
</ul>
<h2 id="场景设计"><a href="#场景设计" class="headerlink" title="场景设计"></a>场景设计</h2><ul>
<li><p>参数化设置</p>
<p><img src="/aposts/d39845c4/image-20211109091445660.png" alt="image-20211109091445660"></p>
</li>
<li><p>回复，发布帖子内容的自定义变量</p>
<p><img src="/aposts/d39845c4/image-20211109091533830.png" alt="image-20211109091533830"></p>
</li>
</ul>
<h3 id="浏览帖子设置"><a href="#浏览帖子设置" class="headerlink" title="浏览帖子设置"></a>浏览帖子设置</h3><ul>
<li><p>共启动40个线程，每隔2秒启动5个线程（表示每个梯次启动 5个线程）且持续运行10秒（每个梯次启动 5个线程）,达到40线程再次运行30秒</p>
</li>
<li><p>可以看到负载预览图是设置后的效果</p>
<p><img src="/aposts/d39845c4/image-20211109093758322.png" alt="image-20211109093758322"></p>
</li>
<li><p><strong>this group will start：</strong>表示总共要启动的线程数；表示总共会加载到 40个线程</p>
</li>
<li><p><strong>first，wait for：</strong>从运行之后多长时间开始启动线程；若设置为 0 秒，表示运行之后立即启动线程</p>
</li>
<li><p><strong>then start：</strong>初次启动多少个线程；若设置为 0 个，表示初次不启动线程</p>
</li>
<li><p><strong>next add：</strong>之后每次启动多少个线程；若设置为 5个，表示每个梯次启动 5个线程</p>
</li>
<li><p><strong>threads every：</strong>当前运行多长时间后再次启动线程，即每一次线程启动完成之后的持续时间；若设置为 10秒，每梯次启动完线程之后再运行 10秒</p>
</li>
<li><p><strong>using ramp-up：</strong>启动线程的时间；若设置为 2秒，表示每次启动线程都持续 2 秒（和基础线程组的ramp-up一样意思）</p>
</li>
<li><p><strong>then hold load for：</strong>线程全部启动完之后持续运行多长时间，设置为 10 秒，表示 40个线程全部启动完之后再持续运行 10秒(以每梯次5次)</p>
</li>
<li><p><strong>finally，stop&#x2F;threads every：</strong>多长时间释放多少个线程；若设置为 5 个和 1 秒，表示持续负载结束之后每 1 秒钟释放 5 个线程，<strong>【注意：线程释放过程中，线程依然在运行】</strong></p>
</li>
</ul>
<h2 id="其他场景"><a href="#其他场景" class="headerlink" title="其他场景"></a>其他场景</h2><ul>
<li>省略</li>
</ul>
<h2 id="Active-Threads-Over-Time"><a href="#Active-Threads-Over-Time" class="headerlink" title="Active Threads Over Time"></a>Active Threads Over Time</h2><ul>
<li>新增监听器，<code>Active Threads Over Time</code>，看到的效果和<code>Stepping Thread Group</code>中的负载预览图一致</li>
</ul>
<p><img src="/aposts/d39845c4/image-20211109094438411.png" alt="image-20211109094438411"></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>这里之前纠结一个奇葩问题，不了解<code>Stepping Thread Group</code>运行机制，手动计算发送请求后和实际发送请求对不上，因此希望其他人不要和我一样</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/1e744d3a/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/1e744d3a/" class="post-title-link" itemprop="url">混合场景单线程组</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-02 18:02:40" itemprop="dateCreated datePublished" datetime="2021-11-02T18:02:40+08:00">2021-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/jmeter/" itemprop="url" rel="index"><span itemprop="name">jmeter</span></a>
                </span>
            </span>

          
            <span id="/aposts/1e744d3a/" class="post-meta-item leancloud_visitors" data-flag-title="混合场景单线程组" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/1e744d3a/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/1e744d3a/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul>
<li>本次混合场景为单个线程组</li>
<li>结合if控制器，计数器，循环控制器</li>
<li>jmeter v5.4.1</li>
<li>自己搭建的jforum</li>
<li>场景为：登录20:浏览帖子:40:回帖:15:发帖10，比例为：5:8:3:2</li>
</ul>
<h2 id="设置线程组"><a href="#设置线程组" class="headerlink" title="设置线程组"></a>设置线程组</h2><ul>
<li>每秒钟并发两个登录请求，循环10次，也就是共发20次登录</li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211102180619354.png" alt="image-20211102180619354"></p>
<ul>
<li><p>参数化数据设置，注意的是线程共享模式</p>
<ul>
<li>所有线程，默认选项，表示按照顺序执行，不会出现多次登录使用同一个用户，<strong>《全栈性能测试修炼宝典JMeter实战》书中作者，自己实现此功能，可能是书中用的jmeter版本比较老，新版本已经有了此功能</strong></li>
<li>当前线程，在这种情况下，每个用户都会从头到尾读取 CSV 文件。</li>
<li>当前线程组，每个文件由每个线程组单独打开（不要与每个线程单独读取文件的“当前线程”混淆。这基本上意味着每个线程组中的每个线程从头到尾读取 CSV 文件，互不影响。要实际展示这种共享模式，您需要再创建一个线程组。</li>
<li>更详细说明请参考<a target="_blank" rel="noopener" href="https://www.blazemeter.com/blog/csv-data-set-config-in-sharing-mode-made-easy">这篇文章</a></li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211103094204881.png" alt="image-20211103094204881"></p>
</li>
<li><p>新增了一个登录计数器</p>
<p><img src="/aposts/1e744d3a/image-20211103095147850.png" alt="image-20211103095147850"></p>
</li>
</ul>
<h2 id="登录简单控制器"><a href="#登录简单控制器" class="headerlink" title="登录简单控制器"></a>登录简单控制器</h2><ul>
<li>登录成功后把模块id提出来</li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211103095319643.png" alt="image-20211103095319643"></p>
<h2 id="进入板块的if控制器"><a href="#进入板块的if控制器" class="headerlink" title="进入板块的if控制器"></a>进入板块的if控制器</h2><ul>
<li><p>按照每登录4次的比例对帖子进行操作</p>
<p><img src="/aposts/1e744d3a/image-20211103095436383.png" alt="image-20211103095436383"></p>
</li>
</ul>
<h3 id="浏览帖子循环控制器"><a href="#浏览帖子循环控制器" class="headerlink" title="浏览帖子循环控制器"></a>浏览帖子循环控制器</h3><ul>
<li>循环8次，刚好为共浏览<code>5*8=40</code>次，提取帖子id，最后进入到帖子详情</li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211103095727165.png" alt="image-20211103095727165"></p>
<h3 id="回复帖子循环控制器"><a href="#回复帖子循环控制器" class="headerlink" title="回复帖子循环控制器"></a>回复帖子循环控制器</h3><ul>
<li>循环次数为3次，共发送<code>3*5=15</code></li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211103100141261.png" alt="image-20211103100141261"></p>
<h3 id="发帖循环控制器"><a href="#发帖循环控制器" class="headerlink" title="发帖循环控制器"></a>发帖循环控制器</h3><ul>
<li>循环2次，共发送<code>2*5=10</code>次</li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211103100343560.png" alt="image-20211103100343560"></p>
<h2 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h2><ul>
<li>发送的请求数量刚好对上</li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211103100519007.png" alt="image-20211103100519007"></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><ul>
<li><p>今天在测试过程中，发现这样的脚本用问题，直接多次回帖和发帖子虽然请求成功了，但是没有插入数据库，用抓包的方式也是如此</p>
</li>
<li><p>后续经过测试，要改成这样的流程，不能单独循环发帖&#x2F;回帖：</p>
<ul>
<li>发帖：进入板块，选择发帖按钮，发送发帖请求</li>
<li>回帖：进入板块，选择帖子，点击回复帖子，发送回帖请求</li>
</ul>
<p><img src="/aposts/1e744d3a/image-20211111175757903.png" alt="image-20211111175757903"></p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://louis-me.github.io/aposts/d312defe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个正经的测试工程师">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="施坤的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/aposts/d312defe/" class="post-title-link" itemprop="url">分布式压测</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-29 15:41:16" itemprop="dateCreated datePublished" datetime="2021-10-29T15:41:16+08:00">2021-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-25 11:20:56" itemprop="dateModified" datetime="2022-02-25T11:20:56+08:00">2022-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/jmeter/" itemprop="url" rel="index"><span itemprop="name">jmeter</span></a>
                </span>
            </span>

          
            <span id="/aposts/d312defe/" class="post-meta-item leancloud_visitors" data-flag-title="分布式压测" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/aposts/d312defe/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/aposts/d312defe/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><ul>
<li>本次实践是基于win虚拟机分布式压测</li>
<li>被压测网站是我自己搭建的jform论坛</li>
</ul>
<h2 id="为什么要压测"><a href="#为什么要压测" class="headerlink" title="为什么要压测"></a>为什么要压测</h2><ul>
<li>需要使用jmeter模拟大并发的情况时，单台压测机不能满足需求，可进行分布式压测。</li>
<li>简单来说就是，多台机器同时安装jmeter，选择一台机器作为调度机，其他作为压力机。进行相应的配置后，就可以用调度机操控压力机发起请求。</li>
</ul>
<h2 id="安装虚拟机"><a href="#安装虚拟机" class="headerlink" title="安装虚拟机"></a>安装虚拟机</h2><ul>
<li><p>官方下载工具</p>
<p><img src="/aposts/d312defe/image-20211029180146221.png" alt="image-20211029180146221"></p>
</li>
<li><p>打开后下载ios文件到本地</p>
</li>
</ul>
<h3 id="安装过程报错"><a href="#安装过程报错" class="headerlink" title="安装过程报错"></a>安装过程报错</h3><ul>
<li>出现一个类似错误</li>
</ul>
<p><img src="/aposts/d312defe/image-20211029180545713.png" alt="image-20211029180545713"></p>
<ul>
<li>解决办法是禁用Hyper-V，以管理员权限打开 CMD 或 Windows PowerShell ，输入如下命令，然后重启电脑</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcdedit /set hypervisorlaunchtype off</span><br></pre></td></tr></table></figure>

<h3 id="网络设置"><a href="#网络设置" class="headerlink" title="网络设置"></a>网络设置</h3><ul>
<li>再虚拟机可以ping通主机的ip，再主机却无法ping通虚拟机ip</li>
<li>经过排查，发现虚拟机ip和主机ip的前面三个网段对不上，VirtualBox设置下虚拟机的网络方式为桥接网卡，就可以了</li>
</ul>
<p><img src="/aposts/d312defe/image-20211029163300356.png" alt="image-20211029163300356"></p>
<h2 id="压力机（master控制机）"><a href="#压力机（master控制机）" class="headerlink" title="压力机（master控制机）"></a>压力机（master控制机）</h2><ul>
<li>我装好的win10虚拟机</li>
<li>执行当前压力机下jmeter安装包bin目录下的jmeter-server的批处理文件，此时该机器上启动一个java进程，并随机分配端口，监听来自调度机的请求。</li>
<li>配置固定端口：打开bin目录下的<code>jmeter.properties</code>文件，更改<code>server_port</code>、<code>server.rmi.localport</code>的端口为要配置的端口。</li>
</ul>
<p><img src="/aposts/d312defe/image-20211029173427158.png" alt="image-20211029173427158"></p>
<ul>
<li><p>第一个ip为本机ip，也可以作为负载机使用</p>
</li>
<li><p>把下面的选项打开设置为true</p>
<p><img src="/aposts/d312defe/image-20211029173713156.png" alt="image-20211029173713156"></p>
</li>
</ul>
<h2 id="调度机（slave负载机）"><a href="#调度机（slave负载机）" class="headerlink" title="调度机（slave负载机）"></a>调度机（slave负载机）</h2><ul>
<li>真实机器</li>
<li>打开jmeter安装包bin目录下的<code>jmeter.properties</code>文件，更改<code>remote_hosts</code>为压力机本机ip及执行jmeter-server后启动的端口。</li>
</ul>
<p><img src="/aposts/d312defe/image-20211029173856668.png" alt="image-20211029173856668"></p>
<ul>
<li>把下面的选项打开设置为true，和master控制机一样</li>
</ul>
<p><img src="/aposts/d312defe/image-20211029173713156.png" alt="image-20211029173713156"></p>
<h2 id="keystore配置"><a href="#keystore配置" class="headerlink" title="keystore配置"></a>keystore配置</h2><ul>
<li><p>这样一来算是配置完成了接下来因为jmeter4.0版本及以上时，为了安全 分布式压测需要一个密匙才能正常实现 打开控制机的bin目录运行create-rmi-keystore.bat，用notepad++打开文件 复制这个命令，使用管理员运行</p>
<p><img src="/aposts/d312defe/image-20211029174237855.png" alt="image-20211029174237855"></p>
</li>
<li><p>执行完后，会出现一个rmi_keystore.jks文件，把这个文件放到master和slave中的jmeter&#x2F;bin目录中</p>
</li>
</ul>
<h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><ul>
<li>运行控制机jmeter.bat  </li>
<li>运行负载机jmeter-server.bat</li>
</ul>
<p><img src="/aposts/d312defe/image-20211029174523893.png" alt="image-20211029174523893"></p>
<ul>
<li>控制机jmeter.bat  ，打开脚本，运行中看到了两个远程启动的地址，点击运行中的远程启动所有，就能看到jmeter-server.bat中日志出现（图片如上）starting（开始发请求）和finished（发送请求结束），<strong>但是我这里的虚拟机一直只有starting的日志，没有finished的日志，造成监控到的请求没有成功，可能是我的笔记本配置太差引起的，后续有条件用真机测试</strong></li>
</ul>
<p><img src="/aposts/d312defe/image-20211029174949327.png" alt="image-20211029174949327"></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li><p>参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/dcszhangsir/p/13463299.html">jmeter分布式压测</a></p>
</li>
<li><p>本机虚拟机一直无法从 starting 转变成 finished没有解决，这里有<a target="_blank" rel="noopener" href="http://testerhome.com/topics/20532">相同问题</a>没有解决</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">


    <div class="sidebar-inner">



      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">一个正经的测试工程师</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">171</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Louis-me" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Louis-me" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shikun.job@foxmail.com" title="E-Mail → mailto:shikun.job@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
  <!-- 标签云 -->

<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
<div class="widget-wrap">
    <!--<h3 class="widget-title">Tag Cloud</h3> -->
    <div id="myCanvasContainer" class="widget tagcloud">
        <canvas width="250" height="250" id="resCanvas" style="width=100%">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AWVS/" rel="tag">AWVS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Android/" rel="tag">Android</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Burp-Site/" rel="tag">Burp Site</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DVWA/" rel="tag">DVWA</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependency-Check/" rel="tag">Dependency-Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/H5/" rel="tag">H5</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hibernate/" rel="tag">Hibernate</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jenkins/" rel="tag">Jenkins</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Metasploit/" rel="tag">Metasploit</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MyBatis/" rel="tag">MyBatis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nessus/" rel="tag">Nessus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sonic/" rel="tag">Sonic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/airtest/" rel="tag">airtest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/android/" rel="tag">android</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/android%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/" rel="tag">android性能测试</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/app%E5%AE%89%E5%85%A8%E6%B5%8B%E8%AF%95/" rel="tag">app安全测试</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/aritest/" rel="tag">aritest</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/centos/" rel="tag">centos</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chrome/" rel="tag">chrome</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/datax/" rel="tag">datax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/drozer/" rel="tag">drozer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/fisco-bcos/" rel="tag">fisco-bcos</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gin/" rel="tag">gin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go/" rel="tag">go</a><span class="tag-list-count">16</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go-zero/" rel="tag">go-zero</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/httprunner/" rel="tag">httprunner</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a><span class="tag-list-count">25</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jmeter/" rel="tag">jmeter</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kail/" rel="tag">kail</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode-%E6%95%B0%E7%BB%84/" rel="tag">leetcode-数组</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mitmproxy/" rel="tag">mitmproxy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/" rel="tag">mongodb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/monkey/" rel="tag">monkey</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/orm/" rel="tag">orm</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/" rel="tag">pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pc%E8%87%AA%E5%8A%A8%E5%8C%96/" rel="tag">pc自动化</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/playwright/" rel="tag">playwright</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytest/" rel="tag">pytest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/selenium/" rel="tag">selenium</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/" rel="tag">spring</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring-cloud/" rel="tag">spring cloud</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/springboot/" rel="tag">springboot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqlite/" rel="tag">sqlite</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqlmap/" rel="tag">sqlmap</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tkinter/" rel="tag">tkinter</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/" rel="tag">vue</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue3/" rel="tag">vue3</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="tag">云服务器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/" rel="tag">区块链</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%89%E5%85%A8%E6%B5%8B%E8%AF%95/" rel="tag">安全测试</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/" rel="tag">微服务</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/" rel="tag">性能测试</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" rel="tag">环境搭建</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" rel="tag">自动化测试</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8B%B1%E6%96%87%E5%AD%A6%E4%B9%A0/" rel="tag">英文学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/" rel="tag">英语学习</a><span class="tag-list-count">1</span></li></ul>
        </canvas>
    </div>
</div>

</div>
<!-- 标签云 -->
    </div>

  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  <a href="https://beian.miit.gov.cn/" target="_blank">湘ICP备2022002212号-1</a><br />
  <!--
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
  -->
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script src='../../../js/src/av-min.js'></script>
<script src='../../../js/src/Valine.min.js'></script>

<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : true,
      notify     : false,
      appId      : 'mhSm1aOQmnVeiAUJxrsKIFg5-gzGzoHsz',
      appKey     : 'kW6GQVoeJac7wDgKvy9vXhn6',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
